Query id: 97
I aim to develop a reinforcement learning framework that utilizes human feedback as sparse reward signals.
This framework will have the capacity to incorporate any model for a variety of potential downstream tasks, such as a generative model for art creation or a chatbot for customer service.
The unique aspect of my framework is its ability to apply reinforcement learning to adjust and optimize the behavior of any downstream model based on various forms of human feedback, such as user satisfaction or the model's adherence to ethical standards.
I understand that obtaining a large amount of human feedback data may be challenging, so my framework needs to be capable of learning from sparse and limited human feedback data.
I am interested in finding out if there has been any previous work in this area.
Furthermore, I would like to know if a reinforcement learning algorithm would still function effectively when the supervised human feedback data is scarce.

Best Candidate ID: 103314, Best Candidate Averaged Relevance Score: 1.909

Best Candidate:
Recent successes combine reinforcement learning algorithms and deep neural
networks, despite reinforcement learning not being widely applied to robotics
and real world scenarios. This can be attributed to the fact that current
state-of-the-art, end-to-end reinforcement learning approaches still require
thousands or millions of data samples to converge to a satisfactory policy and
are subject to catastrophic failures during training. Conversely, in real world
scenarios and after just a few data samples, humans are able to either provide
demonstrations of the task, intervene to prevent catastrophic actions, or
simply evaluate if the policy is performing correctly. This research
investigates how to integrate these human interaction modalities to the
reinforcement learning loop, increasing sample efficiency and enabling
real-time reinforcement learning in robotics and real world scenarios. This
novel theoretical foundation is called Cycle-of-Learning, a reference to how
different human interaction modalities, namely, task demonstration,
intervention, and evaluation, are cycled and combined to reinforcement learning
algorithms. Results presented in this work show that the reward signal that is
learned based upon human interaction accelerates the rate of learning of
reinforcement learning algorithms and that learning from a combination of human
demonstrations and interventions is faster and more sample efficient when
compared to traditional supervised learning algorithms. Finally,
Cycle-of-Learning develops an effective transition between policies learned
using human demonstrations and interventions to reinforcement learning. The
theoretical foundation developed by this research opens new research paths to
human-agent teaming scenarios where autonomous agents are able to learn from
human teammates and adapt to mission performance metrics in real-time and in
real world scenarios.

===========================================================================
Query id: 61
I am working on a natural language processing project focused on entity recognition, which necessitates the comprehension and processing of vast amounts of text data.
My goal is to integrate real-world knowledge during the training phase to bolster the model's performance across various tasks, thus bridging the gap between raw text and meaningful entities.
Therefore, I am in search of models that can generate and learn embeddings for words and entities by leveraging extensive real-world knowledge bases.
Naturally, the model needs to be able to process knowledge and concept in some differentiable way.
Ideally, the trained model should excel in performance on entity-centric datasets and produce competitive results on other standard benchmark datasets.

Best Candidate ID: 6309, Best Candidate Averaged Relevance Score: 1.786

Best Candidate:
The inception of modeling contextual information using models such as BERT,
ELMo, and Flair has significantly improved representation learning for words.
It has also given SOTA results in almost every NLP task - Machine Translation,
Text Summarization and Named Entity Recognition, to name a few. In this work,
in addition to using these dominant context-aware representations, we propose a
Knowledge Aware Representation Learning (KARL) Network for Named Entity
Recognition (NER). We discuss the challenges of using existing methods in
incorporating world knowledge for NER and show how our proposed methods could
be leveraged to overcome those challenges. KARL is based on a Transformer
Encoder that utilizes large knowledge bases represented as fact triplets,
converts them to a graph context, and extracts essential entity information
residing inside to generate contextualized triplet representation for feature
augmentation. Experimental results show that the augmentation done using KARL
can considerably boost the performance of our NER system and achieve
significantly better results than existing approaches in the literature on
three publicly available NER datasets, namely CoNLL 2003, CoNLL++, and
OntoNotes v5. We also observe better generalization and application to a
real-world setting from KARL on unseen entities.

===========================================================================
Query id: 54
I aim to enhance the expressive power of Graph Neural Networks (GNNs) to differentiate between graphs that are structurally different but possess the same local neighborhood structure, a task they traditionally struggle with.
To achieve this, I plan to develop structure-related features that can aid GNNs in representing any set of nodes, thereby increasing their expressive power.
Could you suggest some standard graph-distance measures, such as the shortest path distance, that could be used to capture the distance between the node set and each individual node in the graph?
Furthermore, I want these features to enhance the ability of GNN nodes to gather information from neighboring nodes for self-updating.
Importantly, these features should leverage the sparse structure of the underlying graph, ensuring computational efficiency and scalability.
Finally, I intend to demonstrate the advantages of my proposed features by testing them on standard network tasks, such as structural role prediction.
Could you also suggest other tasks where these features could be tested?

Best Candidate ID: 184286, Best Candidate Averaged Relevance Score: 1.778

Best Candidate:
Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to
measure the expressive power of graph neural networks (GNN). It was shown that
the popular message passing GNN cannot distinguish between graphs that are
indistinguishable by the 1-WL test (Morris et al. 2018; Xu et al. 2019).
Unfortunately, many simple instances of graphs are indistinguishable by the
1-WL test.
  In search for more expressive graph learning models we build upon the recent
k-order invariant and equivariant graph neural networks (Maron et al. 2019a,b)
and present two results:
  First, we show that such k-order networks can distinguish between
non-isomorphic graphs as good as the k-WL tests, which are provably stronger
than the 1-WL test for k>2. This makes these models strictly stronger than
message passing models. Unfortunately, the higher expressiveness of these
models comes with a computational cost of processing high order tensors.
  Second, setting our goal at building a provably stronger, simple and scalable
model we show that a reduced 2-order network containing just scaled identity
operator, augmented with a single quadratic operation (matrix multiplication)
has a provable 3-WL expressive power. Differently put, we suggest a simple
model that interleaves applications of standard Multilayer-Perceptron (MLP)
applied to the feature dimension and matrix multiplication. We validate this
model by presenting state of the art results on popular graph classification
and regression tasks. To the best of our knowledge, this is the first practical
invariant/equivariant model with guaranteed 3-WL expressiveness, strictly
stronger than message passing models.

===========================================================================
Query id: 66
I am currently engaged in a project that involves the generation of realistic text using large-scale, pre-trained language models.
However, I am encountering difficulties in controlling the results of the generation process.
Existing methods, such as prompting, do not offer the level of control I require.
I am in search of an innovative approach that can enhance the controllability of text generation while simultaneously preserving the quality of the text produced.
More specifically, I am considering the projection of certain constraints and control conditions into the latent space.
By injecting these embeddings into the language model's inputs, I anticipate that it could regulate the output of the language model.
I am particularly interested in applying this method to tasks such as constrained text generation, where explicit constraints are present.
My goal is for my proposed method to surpass the performance of other fine-tuned or prompting-based methods.

Best Candidate ID: 22231, Best Candidate Averaged Relevance Score: 1.769

Best Candidate:
Large pre-trained language models have repeatedly shown their ability to
produce fluent text. Yet even when starting from a prompt, generation can
continue in many plausible directions. Current decoding methods with the goal
of controlling generation, e.g., to ensure specific words are included, either
require additional models or fine-tuning, or work poorly when the task at hand
is semantically unconstrained, e.g., story generation. In this work, we present
a plug-and-play decoding method for controlled language generation that is so
simple and intuitive, it can be described in a single sentence: given a topic
or keyword, we add a shift to the probability distribution over our vocabulary
towards semantically similar words. We show how annealing this distribution can
be used to impose hard constraints on language generation, something no other
plug-and-play method is currently able to do with SOTA language generators.
Despite the simplicity of this approach, we see it works incredibly well in
practice: decoding from GPT-2 leads to diverse and fluent sentences while
guaranteeing the appearance of given guide words. We perform two user studies,
revealing that (1) our method outperforms competing methods in human
evaluations; and (2) forcing the guide words to appear in the generated text
has no impact on the fluency of the generated text.

===========================================================================
Query id: 57
It is understood that training-based models can potentially discriminate against individuals in underrepresented minority demographics.
Consequently, I am keen to ensure that machine learning and deep learning models do not foster unfair representations of minority groups.
This could be achieved either by introducing constraints during training or by eliminating discriminatory behaviors during model inference.
Specifically, I am interested in identifying datasets where a traditionally trained model could exhibit discriminatory behavior.
I am also seeking to understand specific terminologies and concepts related to this field of study.
Moreover, I am looking for comprehensive evaluation benchmarks that can help determine the fairness of a model.
My current hypothesis is that if we can pinpoint the decision boundary of a neural model, we can identify where discrimination occurs.
If we can then adjust or modify this decision boundary, we can retroactively correct such discriminatory behaviors in models.
Ultimately, my goal is to develop machine learning algorithms that are not only accurate but also fair and transparent in their treatment of protected groups.

Best Candidate ID: 163198, Best Candidate Averaged Relevance Score: 1.733

Best Candidate:
Machine learning models are widely adopted in scenarios that directly affect
people. The development of software systems based on these models raises
societal and legal concerns, as their decisions may lead to the unfair
treatment of individuals based on attributes like race or gender. Data
preparation is key in any machine learning pipeline, but its effect on fairness
is yet to be studied in detail. In this paper, we evaluate how the fairness and
effectiveness of the learned models are affected by the removal of the
sensitive attribute, the encoding of the categorical attributes, and instance
selection methods (including cross-validators and random undersampling). We
used the Adult Income and the German Credit Data datasets, which are widely
studied and known to have fairness concerns. We applied each data preparation
technique individually to analyse the difference in predictive performance and
fairness, using statistical parity difference, disparate impact, and the
normalised prejudice index. The results show that fairness is affected by
transformations made to the training data, particularly in imbalanced datasets.
Removing the sensitive attribute is insufficient to eliminate all the
unfairness in the predictions, as expected, but it is key to achieve fairer
models. Additionally, the standard random undersampling with respect to the
true labels is sometimes more prejudicial than performing no random
undersampling.

===========================================================================
Query id: 62
As a linguist, I aim to efficiently categorize various semantic elements within a sentence.
For instance, I want to pinpoint the predicate-argument structure.
I plan to create a model based on neural networks that enhances traditional methods which utilize syntactic trees.
Rather than directly employing syntactic trees, my model will learn to convert syntax and grammatical structure into vectorized representations.
These hidden representations will allow my model to be differentiable and converge rapidly.
I'm also curious to see if using a semantic representation of my text from a large pre-trained language model could boost the accuracy of my labeling.
Ultimately, I intend to test my model against a wide range of linguistic benchmarks.

Best Candidate ID: 101065, Best Candidate Averaged Relevance Score: 1.727

Best Candidate:
Semantic role labeling (SRL) is dedicated to recognizing the semantic
predicate-argument structure of a sentence. Previous studies in terms of
traditional models have shown syntactic information can make remarkable
contributions to SRL performance; however, the necessity of syntactic
information was challenged by a few recent neural SRL studies that demonstrate
impressive performance without syntactic backbones and suggest that syntax
information becomes much less important for neural semantic role labeling,
especially when paired with recent deep neural network and large-scale
pre-trained language models. Despite this notion, the neural SRL field still
lacks a systematic and full investigation on the relevance of syntactic
information in SRL, for both dependency and both monolingual and multilingual
settings. This paper intends to quantify the importance of syntactic
information for neural SRL in the deep learning framework. We introduce three
typical SRL frameworks (baselines), sequence-based, tree-based, and
graph-based, which are accompanied by two categories of exploiting syntactic
information: syntax pruning-based and syntax feature-based. Experiments are
conducted on the CoNLL-2005, 2009, and 2012 benchmarks for all languages
available, and results show that neural SRL models can still benefit from
syntactic information under certain conditions. Furthermore, we show the
quantitative significance of syntax to neural SRL models together with a
thorough empirical survey using existing models.

===========================================================================
Query id: 53
In the field of robotic manipulation, I have consistently grappled with the challenge of deploying robots in unfamiliar and evolving environments.
These environments could range from outdoor settings to busy traffic roads or even battlefields.
My objective is to enable my robot to operate efficiently in these new surroundings, acquiring and applying new knowledge to better adapt to the environment.
My current approach is based on the premise that a diverse environment offers a variety of rewards.
Consequently, my robotic agent should be capable of learning to optimize a diverse set of rewards.
Moreover, the agent should be able to swiftly adapt when the rewards (or objectives) change, especially when the environment remains constant.
Ultimately, this system should empower robots to learn from and adapt to changes in their environment and tasks.
This would involve incrementally constructing models based on their own experiences, thereby enhancing their overall performance in real-world scenarios.

Best Candidate ID: 129794, Best Candidate Averaged Relevance Score: 1.7

Best Candidate:
One of the great promises of robot learning systems is that they will be able
to learn from their mistakes and continuously adapt to ever-changing
environments. Despite this potential, most of the robot learning systems today
are deployed as a fixed policy and they are not being adapted after their
deployment. Can we efficiently adapt previously learned behaviors to new
environments, objects and percepts in the real world? In this paper, we present
a method and empirical evidence towards a robot learning framework that
facilitates continuous adaption. In particular, we demonstrate how to adapt
vision-based robotic manipulation policies to new variations by fine-tuning via
off-policy reinforcement learning, including changes in background, object
shape and appearance, lighting conditions, and robot morphology. Further, this
adaptation uses less than 0.2% of the data necessary to learn the task from
scratch. We find that our approach of adapting pre-trained policies leads to
substantial performance gains over the course of fine-tuning, and that
pre-training via RL is essential: training from scratch or adapting from
supervised ImageNet features are both unsuccessful with such small amounts of
data. We also find that these positive results hold in a limited continual
learning setting, in which we repeatedly fine-tune a single lineage of policies
using data from a succession of new tasks. Our empirical conclusions are
consistently supported by experiments on simulated manipulation tasks, and by
52 unique fine-tuning experiments on a real robotic grasping system pre-trained
on 580,000 grasps.

===========================================================================
Query id: 89
I am seeking a generative modeling approach capable of creating new levels and potentially game settings/environments for a video game with multiple existing levels of difficulty.
Specifically, I am interested in exploring how Generative Adversarial Networks (GANs) and other generative methods could generate entirely new levels by emulating the style of previous ones.
It is crucial that the newly generated levels are not merely derivative and that my generative model can optimize specific properties, such as the intensity or graphic nature of the game.
Given that these properties are non-differentiable, I need a method to either render them differentiable or employ a reinforcement learning-centric approach to optimize these rewards.
After generating a variety of levels, I require a method to select some of the best ones.
One potential solution could be to evaluate the generated levels using an automatic metric, such as the performance of an AI agent playing the level.
Alternatively, I am considering designing a derivative-free stochastic optimization algorithm to guide the search across the space of all synthetically generated levels, steering towards those that meet specific objectives.

Best Candidate ID: 112786, Best Candidate Averaged Relevance Score: 1.667

Best Candidate:
Generative adversarial networks (GANs) are quickly becoming a ubiquitous
approach to procedurally generating video game levels. While GAN generated
levels are stylistically similar to human-authored examples, human designers
often want to explore the generative design space of GANs to extract
interesting levels. However, human designers find latent vectors opaque and
would rather explore along dimensions the designer specifies, such as number of
enemies or obstacles. We propose using state-of-the-art quality diversity
algorithms designed to optimize continuous spaces, i.e. MAP-Elites with a
directional variation operator and Covariance Matrix Adaptation MAP-Elites, to
efficiently explore the latent space of a GAN to extract levels that vary
across a set of specified gameplay measures. In the benchmark domain of Super
Mario Bros, we demonstrate how designers may specify gameplay measures to our
system and extract high-quality (playable) levels with a diverse range of level
mechanics, while still maintaining stylistic similarity to human authored
examples. An online user study shows how the different mechanics of the
automatically generated levels affect subjective ratings of their perceived
difficulty and appearance.

===========================================================================
Query id: 3
I want to introduce an operation planning system for transportation authorities and truck companies.
This system will coordinate between different truck companies and transportation authorities to make a driving plan for each truck, for example including when it should move, when it should stop and refuel, and in what route it should be on.
This system will optimize the safety for truck and truck drivers because it will identify highway sections that are crowded.
Therefore, currently I am thinking about using data mining approaches to learn patterns in truck routes.
These approaches could also identify sets of trucks that tend to move together and where they are headed.
I am planning to use spatial temporal data about truck movements from real-life truck fleet planning systems in Liaoning, China, which is readily available.
From this dataset, I hope to  determine the feasibility of coordinating spontaneous truck platoons through speed adjustment.
Overall, the system should also reduce total fuel consumption to achieve the goal of energy savings.

Best Candidate ID: 309656, Best Candidate Averaged Relevance Score: 1.636

Best Candidate:
Freight transportation is of outmost importance for our society and is
continuously increasing. At the same time, transporting goods on roads accounts
for about 26% of all energy consumption and 18% of all greenhouse gas emissions
in the European Union. Despite the influence the transportation system has on
our energy consumption and the environment, road transportation is mainly done
by individual long-haulage trucks with no real-time coordination or global
optimization. In this paper, we review how modern information and communication
technology supports a cyber-physical transportation system architecture with an
integrated logistic system coordinating fleets of trucks traveling together in
vehicle platoons. From the reduced air drag, platooning trucks traveling close
together can save about 10% of their fuel consumption. Utilizing road grade
information and vehicle-to-vehicle communication, a safe and fuel-optimized
cooperative look-ahead control strategy is implemented on top of the existing
cruise controller. By optimizing the interaction between vehicles and platoons
of vehicles, it is shown that significant improvements can be achieved. An
integrated transport planning and vehicle routing in the fleet management
system allows both small and large fleet owners to benefit from the
collaboration. A realistic case study with 200 heavy-duty vehicles performing
transportation tasks in Sweden is described. Simulations show overall fuel
savings at more than 5% thanks to coordinated platoon planning. It is also
illustrated how well the proposed cooperative look-ahead controller for
heavy-duty vehicle platoons manages to optimize the velocity profiles of the
vehicles over a hilly segment of the considered road network.

===========================================================================
Query id: 19
I need to simplify legal documents for individuals with limited English proficiency.
To do this, I plan to train a lexical simplification model that can identify complex words in a given sentence and replace them with simpler alternatives while maintaining the sentence’s meaning.
I want to find a model that takes a complex word and its context as input and generates reasonable candidate substitutions that do not change the meaning of the sentence.
I also want to find state-of-the-art models that can predict the masked words from its context as a reference of my model when generating candidate substitutions.
For all these models, I want both supervised and unsupervised ones.
By comparing the performance of these models, I can decide whether my model should be a supervised one or not.

Best Candidate ID: 212887, Best Candidate Averaged Relevance Score: 1.636

Best Candidate:
Sentence simplification aims to reduce the complexity of a sentence while
retaining its original meaning. Current models for sentence simplification
adopted ideas from ma- chine translation studies and implicitly learned
simplification mapping rules from normal- simple sentence pairs. In this paper,
we explore a novel model based on a multi-layer and multi-head attention
architecture and we pro- pose two innovative approaches to integrate the Simple
PPDB (A Paraphrase Database for Simplification), an external paraphrase
knowledge base for simplification that covers a wide range of real-world
simplification rules. The experiments show that the integration provides two
major benefits: (1) the integrated model outperforms multiple state- of-the-art
baseline models for sentence simplification in the literature (2) through
analysis of the rule utilization, the model seeks to select more accurate
simplification rules. The code and models used in the paper are available at
https://github.com/ Sanqiang/text_simplification.

===========================================================================
Query id: 6
I find that when I need to train a NLP model for a task such as speech recognition, I usually need multiple human annotations per example and I need to aggregate them in order to get some ground-truth.
To reduce the need for human annotation and aggregation, I plan to develop an automated model that can produce aggregated annotation and I will use this improved annotated dataset to train more accurate machine learning models.
To achieve this, I plan on using different automatic annotation toolkits and make multiple annotations for each example.
I also need to take into account the difficulties of examples and I need to learn some representations of each annotation tool (because some may be better than the others).
I would like the annotation procedure to be integrated with the training of machine learning models, so that the entire framework is end-to-end.
To achieve this, I would like to explore different aggregation methods.
Because we are taking multiple annotations per example, there are likely some redundancies, so what would be the best way to guarantee annotation accuracy while reducing annotation redundancy (in other words, taking as few annotations as possible)?

Best Candidate ID: 24719, Best Candidate Averaged Relevance Score: 1.632

Best Candidate:
Training NLP systems typically assumes access to annotated data that has a
single human label per example. Given imperfect labeling from annotators and
inherent ambiguity of language, we hypothesize that single label is not
sufficient to learn the spectrum of language interpretation. We explore new
annotation distribution schemes, assigning multiple labels per example for a
small subset of training examples. Introducing such multi label examples at the
cost of annotating fewer examples brings clear gains on natural language
inference task and entity typing task, even when we simply first train with a
single label data and then fine tune with multi label examples. Extending a
MixUp data augmentation framework, we propose a learning algorithm that can
learn from training examples with different amount of annotation (with zero,
one, or multiple labels). This algorithm efficiently combines signals from
uneven training data and brings additional gains in low annotation budget and
cross domain settings. Together, our method achieves consistent gains in two
tasks, suggesting distributing labels unevenly among training examples can be
beneficial for many NLP tasks.

===========================================================================
Query id: 75
I am currently engaged in a project aimed at identifying critical areas within medical images, such as X-rays, to aid in disease diagnosis.
For instance, in lung X-rays, the region with a tumor is of greater significance, while other areas may not be as relevant.
I am seeking a method to pinpoint these crucial areas in the images.
Subsequently, we can enhance efficiency by capturing high-resolution images of these important areas.
I am interested in models that can execute this concept by initially analyzing a low-resolution image, detecting significant features, and then focusing on more localized but detailed aspects.
Additionally, I require techniques that can deliver the same performance level but with reduced computational power and storage, considering the vast quantity of medical images.
I need a newer and larger dataset to train and test the model.
I would also appreciate recommendations for other image analysis models that are more efficient yet still yield accurate results.

Best Candidate ID: 73684, Best Candidate Averaged Relevance Score: 1.615

Best Candidate:
To enable a deep learning-based system to be used in the medical domain as a
computer-aided diagnosis system, it is essential to not only classify diseases
but also present the locations of the diseases. However, collecting
instance-level annotations for various thoracic diseases is expensive.
Therefore, weakly supervised localization methods have been proposed that use
only image-level annotation. While the previous methods presented the disease
location as the most discriminative part for classification, this causes a deep
network to localize wrong areas for indistinguishable X-ray images. To solve
this issue, we propose a spatial attention method using disease masks that
describe the areas where diseases mainly occur. We then apply the spatial
attention to find the precise disease area by highlighting the highest
probability of disease occurrence. Meanwhile, the various sizes, rotations and
noise in chest X-ray images make generating the disease masks challenging. To
reduce the variation among images, we employ an alignment module to transform
an input X-ray image into a generalized image. Through extensive experiments on
the NIH-Chest X-ray dataset with eight kinds of diseases, we show that the
proposed method results in superior localization performances compared to
state-of-the-art methods.

===========================================================================
Query id: 77
My current job involves efficient image retrieval.
The primary issue is that images require substantial storage space, and the constant conversion to and from compact code results in unnecessary time expenditure.
Therefore, I am seeking a model capable of retrieving images in compact code format.
The challenge for such a model is that images in compact code do not always maintain close local information, leading to increased computational costs for local features.
Consequently, I am interested in image retrieval algorithms and innovative loss functions that prioritize global features.
Additionally, I require algorithms that can add labels or annotations directly to the images’ compact code without necessitating any format conversion of the image.
I prefer methods that have demonstrated effective performance on popular image retrieval benchmarks.

Best Candidate ID: 212048, Best Candidate Averaged Relevance Score: 1.583

Best Candidate:
It has been shown that image descriptors extracted by convolutional neural
networks (CNNs) achieve remarkable results for retrieval problems. In this
paper, we apply attention mechanism to CNN, which aims at enhancing more
relevant features that correspond to important keypoints in the input image.
The generated attention-aware features are then aggregated by the previous
state-of-the-art generalized mean (GeM) pooling followed by normalization to
produce a compact global descriptor, which can be efficiently compared to other
image descriptors by the dot product. An extensive comparison of our proposed
approach with state-of-the-art methods is performed on the new challenging
ROxford5k and RParis6k retrieval benchmarks. Results indicate significant
improvement over previous work. In particular, our attention-aware GeM (AGeM)
descriptor outperforms state-of-the-art method on ROxford5k under the `Hard'
evaluation protocal.

===========================================================================
Query id: 94
I am in the process of developing a deep learning model designed to solve a range of tasks, each with its unique structure.
I am operating under the assumption that the structures of these various tasks can be represented by graphs.
Additionally, I am working with the constraint of having limited labeled data for each task.
My objective is twofold.
Firstly, I aim to create a model capable of processing graphical data, learning from diverse tasks, and then adapting to new tasks using a few-shot learning approach.
My current strategy involves the use of a graph meta-learning algorithm.
Secondly, I have observed that when a sub-task is adversarially designed, it can severely disrupt the meta-learning algorithm.
As a result, I need to ensure my model is robust against adversarial attacks.
My current strategy to achieve this involves introducing randomized variations in the model architecture, input data, and intermediate latent results to fortify my model.
I am also open to other robust training strategies.
By integrating these techniques, my goal is to develop a scalable and robust deep learning model that can efficiently adapt to new graph-based tasks and resist adversarial attacks.

Best Candidate ID: 251741, Best Candidate Averaged Relevance Score: 1.583

Best Candidate:
Learning tasks on source code (i.e., formal languages) have been considered
recently, but most work has tried to transfer natural language methods and does
not capitalize on the unique opportunities offered by code's known syntax. For
example, long-range dependencies induced by using the same variable or function
in distant locations are often not considered. We propose to use graphs to
represent both the syntactic and semantic structure of code and use graph-based
deep learning methods to learn to reason over program structures.
  In this work, we present how to construct graphs from source code and how to
scale Gated Graph Neural Networks training to such large graphs. We evaluate
our method on two tasks: VarNaming, in which a network attempts to predict the
name of a variable given its usage, and VarMisuse, in which the network learns
to reason about selecting the correct variable that should be used at a given
program location. Our comparison to methods that use less structured program
representations shows the advantages of modeling known structure, and suggests
that our models learn to infer meaningful names and to solve the VarMisuse task
in many cases. Additionally, our testing showed that VarMisuse identifies a
number of bugs in mature open-source projects.

===========================================================================
Query id: 70
I am currently engaged in a project that involves compressing High Dynamic Range (HDR) images to be compatible with older JPEG formats.
I have found that the existing JPEG XT standard, which is designed to support high dynamic range imaging, does not fully accommodate HDR images.
Consequently, I am in need of a model that can convert HDR images into a JPEG XT version with minimal loss of information.
I am interested in learning how to incorporate image compression methods into this task.
My objective is to develop a universal model that can effectively handle this task without the need to find the optimal setting for each individual image.
I am seeking guidance on how to design and train such a model, including the selection of appropriate training data and suitable performance metrics.
I anticipate that a neural network-based compression algorithm could potentially outperform traditional algorithms.
Therefore, I would appreciate any insights into the compression methods based on CNN, autoencoders, or GANs.

Best Candidate ID: 230145, Best Candidate Averaged Relevance Score: 1.571

Best Candidate:
Detection of contrast adjustments in the presence of JPEG postprocessing is
known to be a challenging task. JPEG post processing is often applied
innocently, as JPEG is the most common image format, or it may correspond to a
laundering attack, when it is purposely applied to erase the traces of
manipulation. In this paper, we propose a CNN-based detector for generic
contrast adjustment, which is robust to JPEG compression. The proposed system
relies on a patch-based Convolutional Neural Network (CNN), trained to
distinguish pristine images from contrast adjusted images, for some selected
adjustment operators of different nature. Robustness to JPEG compression is
achieved by training the CNN with JPEG examples, compressed over a range of
Quality Factors (QFs). Experimental results show that the detector works very
well and scales well with respect to the adjustment type, yielding very good
performance under a large variety of unseen tonal adjustments.

===========================================================================
Query id: 20
I am working for a social media platform and my task is to automatically generate captions for user-uploaded images so that overall user experience is enhanced.
I know that computer vision models can effectively do image captioning but for similar images they tend to give out the same vanilla and bland descriptions.
I need to be building a model that could introduce diversity into the process of image captioning.
I want my model to be able to  generate distinct captions for similar images.
Some ideas that I have so far include: using a customized weighted loss function so that the model will be penalized for bland descriptions.
Secondly, use reinforcement learning and treat diversity as reward in the training process.
Thirdly, I could also group similar images together as a training bundle.
I also need some new metrics to evaluate the distinctiveness of the captions.
Finally, I would like my model to outperform state-of-the-art image captioning models in terms of diversity with minimal degradation in accuracy.

Best Candidate ID: 110600, Best Candidate Averaged Relevance Score: 1.556

Best Candidate:
Significant progress has been made in recent years in image captioning, an
active topic in the fields of vision and language. However, existing methods
tend to yield overly general captions and consist of some of the most frequent
words/phrases, resulting in inaccurate and indistinguishable descriptions (see
Figure 1). This is primarily due to (i) the conservative characteristic of
traditional training objectives that drives the model to generate correct but
hardly discriminative captions for similar images and (ii) the uneven word
distribution of the ground-truth captions, which encourages generating highly
frequent words/phrases while suppressing the less frequent but more concrete
ones. In this work, we propose a novel global-local discriminative objective
that is formulated on top of a reference model to facilitate generating
fine-grained descriptive captions. Specifically, from a global perspective, we
design a novel global discriminative constraint that pulls the generated
sentence to better discern the corresponding image from all others in the
entire dataset. From the local perspective, a local discriminative constraint
is proposed to increase attention such that it emphasizes the less frequent but
more concrete words/phrases, thus facilitating the generation of captions that
better describe the visual details of the given images. We evaluate the
proposed method on the widely used MS-COCO dataset, where it outperforms the
baseline methods by a sizable margin and achieves competitive performance over
existing leading approaches. We also conduct self-retrieval experiments to
demonstrate the discriminability of the proposed method.

===========================================================================
Query id: 35
I am a sales manager, and I usually need to spend a long time extracting the data I need from numerous files and tables and then finding out an effective visual presentation for the sales data.
I am looking for a system that can make my life easier.
I need a system that can extract data from identified tables, and store the data in a structured format.
Besides data mining, I need the system to analyze the data and generate multiple visual presentation options.
Then, the system can evaluate the effectiveness of the options and select the most appropriate visualization.
I have already conducted a survey in the company, and I have sufficient data to train or retrain a system that can do the job.

Best Candidate ID: 316737, Best Candidate Averaged Relevance Score: 1.556

Best Candidate:
The exploding growth of digital data in the information era and its
immeasurable potential value has called for different types of data-driven
techniques to exploit its value for further applications. Information
visualization and data mining are two research field with such goal. While the
two communities advocates different approaches of problem solving, the vision
of combining the sophisticated algorithmic techniques from data mining as well
as the intuitivity and interactivity of information visualization is tempting.
In this paper, we attempt to survey recent researches and real world systems
integrating the wisdom in two fields towards more effective and efficient data
analytics. More specifically, we study the intersection from a data mining
point of view, explore how information visualization can be used to complement
and improve different stages of data mining through established theories for
optimized visual presentation as well as practical toolsets for rapid
development. We organize the survey by identifying three main stages of typical
process of data mining, the preliminary analysis of data, the model
construction, as well as the model evaluation, and study how each stage can
benefit from information visualization.

===========================================================================
Query id: 14
Though data augmentation can be effective to improve model's performances, for natural language models, data augmentation can also cause noises in model’s output such as prediction or machine translation.
My research fields involve training models for text classification, question answering and sequence labeling, all of which could benefit from using augmented dataset.
It is important for me to understand how data augmentations could cause noises in predictions.
I want to identify what augmentation technique could cause noises in prediction, so I should perform a sensitivity analysis.
Overall, I want to penalize the model if there are too much noises in the output due to data augmentation.
My current idea is to use some form of consistency regularization in my loss function during training or fine-tuning.
In the end, I want to see improved performance of my trained models with consistency regularization on benchmark datasets for NLP tasks.

Best Candidate ID: 188492, Best Candidate Averaged Relevance Score: 1.538

Best Candidate:
Semi-supervised learning lately has shown much promise in improving deep
learning models when labeled data is scarce. Common among recent approaches is
the use of consistency training on a large amount of unlabeled data to
constrain model predictions to be invariant to input noise. In this work, we
present a new perspective on how to effectively noise unlabeled examples and
argue that the quality of noising, specifically those produced by advanced data
augmentation methods, plays a crucial role in semi-supervised learning. By
substituting simple noising operations with advanced data augmentation methods
such as RandAugment and back-translation, our method brings substantial
improvements across six language and three vision tasks under the same
consistency training framework. On the IMDb text classification dataset, with
only 20 labeled examples, our method achieves an error rate of 4.20,
outperforming the state-of-the-art model trained on 25,000 labeled examples. On
a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms
all previous approaches and achieves an error rate of 5.43 with only 250
examples. Our method also combines well with transfer learning, e.g., when
finetuning from BERT, and yields improvements in high-data regime, such as
ImageNet, whether when there is only 10% labeled data or when a full labeled
set with 1.3M extra unlabeled examples is used. Code is available at
https://github.com/google-research/uda.

===========================================================================
Query id: 55
I am seeking to comprehend the performance of the MAML algorithm in meta-learning when confronted with adversarial attacks aimed at contaminating its learned parameters.
Initially, I require a method to swiftly generate examples of adversarial attacks.
Subsequently, I am interested in investigating defense strategies that enable MAML to either identify adversarial examples or prevent learning from them.
My current approach involves implementing some form of regularization to ensure that MAML is not disproportionately influenced by a handful of malicious data examples.
Given that MAML can perform parameter updates in either its inner or outer loop, it is crucial for me to determine the optimal location to deploy these defense strategies, such as regularization.
Ultimately, I aim to develop an end-to-end framework capable of automatically assessing the stability of MAML and other meta-learning algorithms when subjected to adversarial attacks.

Best Candidate ID: 114784, Best Candidate Averaged Relevance Score: 1.538

Best Candidate:
As we seek to deploy machine learning models beyond virtual and controlled
domains, it is critical to analyze not only the accuracy or the fact that it
works most of the time, but if such a model is truly robust and reliable. This
paper studies strategies to implement adversary robustly trained algorithms
towards guaranteeing safety in machine learning algorithms. We provide a
taxonomy to classify adversarial attacks and defenses, formulate the Robust
Optimization problem in a min-max setting and divide it into 3 subcategories,
namely: Adversarial (re)Training, Regularization Approach, and Certified
Defenses. We survey the most recent and important results in adversarial
example generation, defense mechanisms with adversarial (re)Training as their
main defense against perturbations. We also survey mothods that add
regularization terms that change the behavior of the gradient, making it harder
for attackers to achieve their objective. Alternatively, we've surveyed methods
which formally derive certificates of robustness by exactly solving the
optimization problem or by approximations using upper or lower bounds. In
addition, we discuss the challenges faced by most of the recent algorithms
presenting future research perspectives.

===========================================================================
Query id: 26
I am developing an application on mobile devices like smartphones that can analyze medical images and detect early signs of cancer.
To do that, I need the App to efficiently process large amounts of visual data with limited time and memory resources.
I am looking for a feature learning algorithm that can simplify the input data while keeping the most critical features in the image.
The algorithm should already be trained on medical images so that it can understand that even subtle differences in some specific parts of the organs are more important.
Also, I need strategies to distill large neural networks into smaller ones by reducing the number of parameters, so that a complex neural network can run on the mobile device.

Best Candidate ID: 174269, Best Candidate Averaged Relevance Score: 1.533

Best Candidate:
Segmentation is a critical step in medical image analysis. Fully
Convolutional Networks (FCNs) have emerged as powerful segmentation models
achieving state-of-the-art results in various medical image datasets. Network
architectures are usually designed manually for a specific segmentation task so
applying them to other medical datasets requires extensive experience and time.
Moreover, the segmentation requires handling large volumetric data that results
in big and complex architectures. Recently, methods that automatically design
neural networks for medical image segmentation have been presented; however,
most approaches either do not fully consider volumetric information or do not
optimize the size of the network. In this paper, we propose a novel
self-adaptive 2D-3D ensemble of FCNs for medical image segmentation that
incorporates volumetric information and optimizes both the model's performance
and size. The model is composed of an ensemble of a 2D FCN that extracts
intra-slice information, and a 3D FCN that exploits inter-slice information.
The architectures of the 2D and 3D FCNs are automatically adapted to a medical
image dataset using a multiobjective evolutionary based algorithm that
minimizes both the segmentation error and number of parameters in the network.
The proposed 2D-3D FCN ensemble was tested on the task of prostate segmentation
on the image dataset from the PROMISE12 Grand Challenge. The resulting network
is ranked in the top 10 submissions, surpassing the performance of other
automatically-designed architectures while being considerably smaller in size.

===========================================================================
Query id: 72
I am currently involved in a project that aims to improve the safety and efficiency of self-driving cars by predicting the paths of surrounding objects in urban driving situations.
Current methods, such as multimodal regression and occupancy maps, have shown limitations in accurately predicting an object’s path.
My plan is to treat trajectory prediction as a ranking task, sorting through the set of all potential paths.
I need algorithms that can generate a diverse set of potential paths, accommodating a wide range of situations while concurrently discarding paths that are physically unfeasible.
To enhance this method, I plan to generate these path sets based on the vehicle's current situation.
For example, when the vehicle is on a highway, the model should not generate paths for objects on the roadside.
I am also interested in integrating real-time traffic data and weather conditions into the model to increase its accuracy.
Lastly, I aim to validate my novel approach using real-world autonomous vehicle data and compare its performance with existing trajectory prediction models.

Best Candidate ID: 181782, Best Candidate Averaged Relevance Score: 1.533

Best Candidate:
Autonomous explorative robots frequently encounter scenarios where multiple
future trajectories can be pursued. Often these are cases with multiple paths
around an obstacle or trajectory options towards various frontiers. Humans in
such situations can inherently perceive and reason about the surrounding
environment to identify several possibilities of either manoeuvring around the
obstacles or moving towards various frontiers. In this work, we propose a 2
stage Convolutional Neural Network architecture which mimics such an ability to
map the perceived surroundings to multiple trajectories that a robot can choose
to traverse. The first stage is a Trajectory Proposal Network which suggests
diverse regions in the environment which can be occupied in the future. The
second stage is a Trajectory Sampling network which provides a finegrained
trajectory over the regions proposed by Trajectory Proposal Network. We
evaluate our framework in diverse and complicated real life settings. For the
outdoor case, we use the KITTI dataset and our own outdoor driving dataset. In
the indoor setting, we use an autonomous drone to navigate various scenarios
and also a ground robot which can explore the environment using the
trajectories proposed by our framework. Our experiments suggest that the
framework is able to develop a semantic understanding of the obstacles, open
regions and identify diverse trajectories that a robot can traverse. Our
comparisons portray the performance gain of the proposed architecture over a
diverse set of methods against which it is compared.

===========================================================================
Query id: 34
In real-life, there are a lot of decisions that need to be made under uncertain conditions.
For example, in autonomous driving, most models are comfortable about making the correct decision with sufficient knowledge about the surrounding environment, but these models usually have problems under cases when they do not have time to collect enough data.
Thus, I need a planning system that can handle uncertainty and incomplete knowledge about the real world.
I am more familiar with the Bayesian models, so I hope the system can represent the possible options and consequences using actions and states, and use a belief network to capture probabilistic relationships between them.
The system should also be able to  reason explicitly about uncertainty using the belief network.
I hope the model is also re-trainable so that I can train the model with specific data that is related to my work.

Best Candidate ID: 215786, Best Candidate Averaged Relevance Score: 1.529

Best Candidate:
Addressing uncertainty is critical for autonomous systems to robustly adapt
to the real world. We formulate the problem of model uncertainty as a
continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent
maintains a posterior distribution over latent model parameters given a history
of observations and maximizes its expected long-term reward with respect to
this belief distribution. Our algorithm, Bayesian Policy Optimization, builds
on recent policy optimization algorithms to learn a universal policy that
navigates the exploration-exploitation trade-off to maximize the Bayesian value
function. To address challenges from discretizing the continuous latent
parameter space, we propose a new policy network architecture that encodes the
belief distribution independently from the observable state. Our method
significantly outperforms algorithms that address model uncertainty without
explicitly reasoning about belief distributions and is competitive with
state-of-the-art Partially Observable Markov Decision Process solvers.

===========================================================================
Query id: 85
I am seeking to develop a robotic agent capable of comprehending user input and completing assigned tasks.
I believe the agent's functionality can be separated into two segments: understanding commands and executing actions.
First, I require a model that can interpret user commands, whether written or verbal, and translate them into a list of real-world actions.
I believe a language model would be beneficial at this stage.
However, I need advice on how to strike a balance between the size of the language model and the accuracy of its interpretation of the input.
Additionally, I require a reinforcement learning framework for the action execution phase.
However, I've noticed that users often provide detailed yet suboptimal instructions.
Therefore, I am in search of a reward function that incorporates social psychology to enhance service quality.
I anticipate an algorithm that empowers the agent to assess user input and carry out the most efficient actions while not straying too far from the original input.

Best Candidate ID: 260092, Best Candidate Averaged Relevance Score: 1.529

Best Candidate:
Robots operating alongside humans in diverse, stochastic environments must be
able to accurately interpret natural language commands. These instructions
often fall into one of two categories: those that specify a goal condition or
target state, and those that specify explicit actions, or how to perform a
given task. Recent approaches have used reward functions as a semantic
representation of goal-based commands, which allows for the use of a
state-of-the-art planner to find a policy for the given task. However, these
reward functions cannot be directly used to represent action-oriented commands.
We introduce a new hybrid approach, the Deep Recurrent Action-Goal Grounding
Network (DRAGGN), for task grounding and execution that handles natural
language from either category as input, and generalizes to unseen environments.
Our robot-simulation results demonstrate that a system successfully
interpreting both goal-oriented and action-oriented task specifications brings
us closer to robust natural language understanding for human-robot interaction.

===========================================================================
Query id: 10
I am working on a project analyzing customer feedback for a new product launch, so I need a method that can effectively identify the topics and segments in each feedback.
Since the feedback we received is very complicated and contains a lot of noise words that digress from main topics, I need a method that is able to remove noise and simplify these feedbacks.
Using this method, I hope our system could provide insights into the most common issues customers are facing and help us improve the product accordingly.
Some ideas I have right now to simplify feedback and remove noise are to leverage word order information and heuristics.
The final resulting system must beat state-of-the-art systems in terms of topic segmentation.

Best Candidate ID: 111207, Best Candidate Averaged Relevance Score: 1.526

Best Candidate:
With the evolution of the cloud and customer centric culture, we inherently
accumulate huge repositories of textual reviews, feedback, and support
data.This has driven enterprises to seek and research engagement patterns, user
network analysis, topic detections, etc.However, huge manual work is still
necessary to mine data to be able to mine actionable outcomes. In this paper,
we proposed and developed an innovative Semi-Supervised Learning approach by
utilizing Deep Learning and Topic Modeling to have a better understanding of
the user voice.This approach combines a BERT-based multiclassification
algorithm through supervised learning combined with a novel Probabilistic and
Semantic Hybrid Topic Inference (PSHTI) Model through unsupervised learning,
aiming at automating the process of better identifying the main topics or areas
as well as the sub-topics from the textual feedback and support.There are three
major break-through: 1. As the advancement of deep learning technology, there
have been tremendous innovations in the NLP field, yet the traditional topic
modeling as one of the NLP applications lag behind the tide of deep learning.
In the methodology and technical perspective, we adopt transfer learning to
fine-tune a BERT-based multiclassification system to categorize the main topics
and then utilize the novel PSHTI model to infer the sub-topics under the
predicted main topics. 2. The traditional unsupervised learning-based topic
models or clustering methods suffer from the difficulty of automatically
generating a meaningful topic label, but our system enables mapping the top
words to the self-help issues by utilizing domain knowledge about the product
through web-crawling. 3. This work provides a prominent showcase by leveraging
the state-of-the-art methodology in the real production to help shed light to
discover user insights and drive business investment priorities.

===========================================================================
Query id: 56
My goal is to develop a learning model that can handle multiple tasks simultaneously.
This proposed learning model will function as a sub-model selector, meaning that when presented with a new task, it will determine the most suitable sub-model to learn this task.
Additionally, my learning model could also operate as a sub-model constructor.
If it determines that no existing sub-model can learn the task, it will generate innovative model architectures to construct a new sub-model suitable for the task.
At present, I am considering the application of a combination of reinforcement learning and model architecture search algorithms to achieve this.
To effectively evaluate the performance, I require a comprehensive benchmark that includes a variety of datasets that can be used as different sub-tasks.
My aim is for my learning model to achieve state-of-the-art performance on this benchmark.
In general, I am open to any methodologies that enhance the performance and adaptability of my learning model across different problem domains.

Best Candidate ID: 194220, Best Candidate Averaged Relevance Score: 1.526

Best Candidate:
Multi-task learning, as it is understood nowadays, consists of using one
single model to carry out several similar tasks. From classifying hand-written
characters of different alphabets to figuring out how to play several Atari
games using reinforcement learning, multi-task models have been able to widen
their performance range across different tasks, although these tasks are
usually of a similar nature. In this work, we attempt to widen this range even
further, by including heterogeneous tasks in a single learning procedure. To do
so, we firstly formally define a multi-network model, identifying the necessary
components and characteristics to allow different adaptations of said model
depending on the tasks it is required to fulfill. Secondly, employing the
formal definition as a starting point, we develop an illustrative model example
consisting of three different tasks (classification, regression and data
sampling). The performance of this model implementation is then analyzed,
showing its capabilities. Motivated by the results of the analysis, we
enumerate a set of open challenges and future research lines over which the
full potential of the proposed model definition can be exploited.

===========================================================================
Query id: 68
In natural language processing tasks, it is essential to accurately represent the meanings of words to achieve optimal performance.
However, this task is challenging due to the potential for multiple interpretations of a single word.
As a solution, I propose a method that initially learns to identify the smallest unambiguous phrase component within sentences and paragraphs.
Following this, I aim to accurately model their latent vectorized representations, taking into account their semantic and structural relationships with neighboring semantic components.
My current approach involves capturing the semantic relationship using a pretraining method akin to word2vec.
To encapsulate the structural syntactic relationships, I propose modeling each semantic component as a node, with the entire paragraph represented as a sparse graph structure.
Subsequently, specialized graph-based neural network algorithms can be applied to obtain latent structural representations.
Ultimately, my proposed embedding model should assign a more precise embedding to each text input, potentially enhancing performance in numerous downstream tasks.

Best Candidate ID: 220576, Best Candidate Averaged Relevance Score: 1.526

Best Candidate:
Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a
sequential LSTM, to extract word order features while neglecting other valuable
syntactic information such as dependency graph or constituent trees. In this
paper, we first propose to use the \textit{syntactic graph} to represent three
types of syntactic information, i.e., word order, dependency and constituency
features. We further employ a graph-to-sequence model to encode the syntactic
graph and decode a logical form. Experimental results on benchmark datasets
show that our model is comparable to the state-of-the-art on Jobs640, ATIS and
Geo880. Experimental results on adversarial examples demonstrate the robustness
of the model is also improved by encoding more syntactic information.

===========================================================================
Query id: 48
With the advent of big data, we often face the challenge of complex heterogeneity within a dataset.
In order to keep user privacy, we would try to avoid sending user data directly to the cloud.
Instead, we want a system working on personal devices that can model complex heterogeneity locally.
We need a heterogeneous learning framework, which combines both the weighted unsupervised contrastive loss and the weighted supervised contrastive loss to model multiple types of heterogeneity.
The framework should also include model distillation techniques such as pruning.
Since we need to run the framework on personal devices like smartphones, the framework should provide some guarantee on performance after distillation.
I am looking for strategies using semi-definite programming to estimate the error bound of the distilled framework.

Best Candidate ID: 24991, Best Candidate Averaged Relevance Score: 1.5

Best Candidate:
Federated learning enables multiple distributed devices to collaboratively
learn a shared prediction model without centralizing their on-device data. Most
of the current algorithms require comparable individual efforts for local
training with the same structure and size of on-device models, which, however,
impedes participation from resource-constrained devices. Given the widespread
yet heterogeneous devices nowadays, in this paper, we propose an innovative
federated learning framework with heterogeneous on-device models through
Zero-shot Knowledge Transfer, named by FedZKT. Specifically, FedZKT allows
devices to independently determine the on-device models upon their local
resources. To achieve knowledge transfer across these heterogeneous on-device
models, a zero-shot distillation approach is designed without any prerequisites
for private on-device data, which is contrary to certain prior research based
on a public dataset or a pre-trained data generator. Moreover, this
compute-intensive distillation task is assigned to the server to allow the
participation of resource-constrained devices, where a generator is
adversarially learned with the ensemble of collected on-device models. The
distilled central knowledge is then sent back in the form of the corresponding
on-device model parameters, which can be easily absorbed on the device side.
Extensive experimental studies demonstrate the effectiveness and robustness of
FedZKT towards on-device knowledge agnostic, on-device model heterogeneity, and
other challenging federated learning scenarios, such as heterogeneous on-device
data and straggler effects.

===========================================================================
Query id: 9
I am a cybersecurity analyst working on designing a system to detect malicious flows and anomalies in cloud environments.
Assuming there are always some malicious flows in the cloud environment, the system will be able to detect almost all of the malicious flows when the cloud environment is under attack and the majority of malicious flows when the environment is not attacked.
To improve the effectiveness of the system, I want to incorporate machine learning models in my methods to help detect all the flows happening in the cloud.
Furthermore, I want the system to recognize different kinds of flows and tell me which flows are benign and which are harmful to my environment.
In addition to finding the flows in my environment, the system needs to provide explanations for the flow detection results.
For generalization purposes, I hope my system can work on different datasets at various cloud locations in my cloud environment.

Best Candidate ID: 28589, Best Candidate Averaged Relevance Score: 1.474

Best Candidate:
In cloud computing, it is desirable if suspicious activities can be detected
by automatic anomaly detection systems. Although anomaly detection has been
investigated in the past, it remains unsolved in cloud computing. Challenges
are: characterizing the normal behavior of a cloud server, distinguishing
between benign and malicious anomalies (attacks), and preventing alert fatigue
due to false alarms.
  We propose CloudShield, a practical and generalizable real-time anomaly and
attack detection system for cloud computing. Cloudshield uses a general,
pretrained deep learning model with different cloud workloads, to predict the
normal behavior and provide real-time and continuous detection by examining the
model reconstruction error distributions. Once an anomaly is detected, to
reduce alert fatigue, CloudShield automatically distinguishes between benign
programs, known attacks, and zero-day attacks, by examining the prediction
error distributions. We evaluate the proposed CloudShield on representative
cloud benchmarks. Our evaluation shows that CloudShield, using model
pretraining, can apply to a wide scope of cloud workloads. Especially, we
observe that CloudShield can detect the recently proposed speculative execution
attacks, e.g., Spectre and Meltdown attacks, in milliseconds. Furthermore, we
show that CloudShield accurately differentiates and prioritizes known attacks,
and potential zero-day attacks, from benign programs. Thus, it significantly
reduces false alarms by up to 99.0%.

===========================================================================
Query id: 64
I am currently developing an application that can efficiently search for images relevant to given text descriptions by integrating both visual and textual data.
Although current state-of-the-art pre-trained models are effective, they are limited by slow inference speeds and substantial computational costs.
These limitations are primarily due to the attention mechanisms in the Transformer architecture, making them impractical for real-world applications where low latency and computational efficiency are crucial.
Consequently, I am seeking a solution that can enhance inference speeds without sacrificing model accuracy.
One potential approach is to represent texts and images in a structured, easily searchable embedding format.
This format should remain a vectorized embedding but could incorporate certain structures that facilitate faster search and ranking.
All these processes should be conducted offline, and during online inference, only highly efficient cosine similarity matching should be performed from their embeddings.
Ultimately, my goal is for this solution to outperform existing pre-trained models in both speed and accuracy across various image and text retrieval benchmarks.

Best Candidate ID: 44781, Best Candidate Averaged Relevance Score: 1.471

Best Candidate:
Conventional approaches to image-text retrieval mainly focus on indexing
visual objects appearing in pictures but ignore the interactions between these
objects. Such objects occurrences and interactions are equivalently useful and
important in this field as they are usually mentioned in the text. Scene graph
presentation is a suitable method for the image-text matching challenge and
obtained good results due to its ability to capture the inter-relationship
information. Both images and text are represented in scene graph levels and
formulate the retrieval challenge as a scene graph matching challenge. In this
paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model
that enhances the state-of-the-art method by integrating an extra graph
convolution network to capture the general information of a graph.
Specifically, for a pair of scene graphs of an image and its caption, two
separate models are used to learn the features of each graph's nodes and edges.
Then a Siamese-structure graph convolution model is employed to embed graphs
into vector forms. We finally combine the graph-level and the vector-level to
calculate the similarity of this image-text pair. The empirical experiments
show that our enhancement with the combination of levels can improve the
performance of the baseline method by increasing the recall by more than 10% on
the Flickr30k dataset.

===========================================================================
Query id: 25
We are making a film, in which I need to project a real person’s facial expression to a virtual character.
I am planning to develop a system for facial and head reenactment that can seamlessly transfer facial expressions, head pose, and eye gaze from a source video to a target subject in nearly real-time speed.
I first need a model that can accurately track facial expressions, head pose and eye gaze from a source video.
I hope the model has very high accuracy supported by test results.
Then, I plan to develop a deep learning model that can analyze 3D geometry of faces and modify facial attributes.
I heard GANs are good for these tasks so references from GANs can be very helpful.

Best Candidate ID: 202313, Best Candidate Averaged Relevance Score: 1.467

Best Candidate:
Teleconference or telepresence based on virtual reality (VR) headmount
display (HMD) device is a very interesting and promising application since HMD
can provide immersive feelings for users. However, in order to facilitate
face-to-face communications for HMD users, real-time 3D facial performance
capture of a person wearing HMD is needed, which is a very challenging task due
to the large occlusion caused by HMD. The existing limited solutions are very
complex either in setting or in approach as well as lacking the performance
capture of 3D eye gaze movement. In this paper, we propose a convolutional
neural network (CNN) based solution for real-time 3D face-eye performance
capture of HMD users without complex modification to devices. To address the
issue of lacking training data, we generate massive pairs of HMD face-label
dataset by data synthesis as well as collecting VR-IR eye dataset from multiple
subjects. Then, we train a dense-fitting network for facial region and an eye
gaze network to regress 3D eye model parameters. Extensive experimental results
demonstrate that our system can efficiently and effectively produce in real
time a vivid personalized 3D avatar with the correct identity, pose, expression
and eye motion corresponding to the HMD user.

===========================================================================
Query id: 63
I am interested in investigating linguistics from a statistical and mathematical standpoint.
My focus is on modeling linguistic patterns and phenomena as components of a complex system.
I aim to compute statistical and information-theoretic features from extensive streams of natural language texts.
Ideally, these features would provide insights into modeling the long-term syntactical and semantic dependencies between various linguistic elements within a lengthy text stream.
Moreover, I am keen on modeling human language as a complex communication system regulated by the principles of information theory.
The ultimate objective is to reveal hidden patterns and enhance or consolidate existing patterns using the language of information theory and statistics.
Therefore, I need a thorough review of recent advancements at the intersection of linguistics and complex systems.

Best Candidate ID: 83703, Best Candidate Averaged Relevance Score: 1.467

Best Candidate:
Traditional linguistic theories have largely regard language as a formal
system composed of rigid rules. However, their failures in processing real
language, the recent successes in statistical natural language processing, and
the findings of many psychological experiments have suggested that language may
be more a probabilistic system than a formal system, and thus cannot be
faithfully modeled with the either/or rules of formal linguistic theory. The
present study, based on authentic language data, confirmed that those important
linguistic issues, such as linguistic universal, diachronic drift, and language
variations can be translated into probability and frequency patterns in parole.
These findings suggest that human language may well be probabilistic systems by
nature, and that statistical may well make inherent properties of human
languages.

===========================================================================
Query id: 36
In the healthcare industry, keeping the patients’ privacy is critical.
Since the amount of data is so large, I need an efficient AI system that can address ethical concerns and uses a machine learning paradigm to identify the private information of the patients and provide robust safeguard to sensitive patient data.
The system must be able to maintain privacy protection under large amounts of data.
I am looking for an open-sourced model that uses a similar framework like Open Health so that I do not need to spend extra time teaching the staff to use a brand-new system.
I heard federated learning is quite popular these days, but it might be hard to find a model using federated learning in the healthcare field since it is very new, so either models using federated learning that is in another field or models does not use federated learning but can effectively protect patient’s privacy work for me.

Best Candidate ID: 114692, Best Candidate Averaged Relevance Score: 1.462

Best Candidate:
The high demand of artificial intelligence services at the edges that also
preserve data privacy has pushed the research on novel machine learning
paradigms that fit those requirements. Federated learning has the ambition to
protect data privacy through distributed learning methods that keep the data in
their data silos. Likewise, differential privacy attains to improve the
protection of data privacy by measuring the privacy loss in the communication
among the elements of federated learning. The prospective matching of federated
learning and differential privacy to the challenges of data privacy protection
has caused the release of several software tools that support their
functionalities, but they lack of the needed unified vision for those
techniques, and a methodological workflow that support their use. Hence, we
present the Sherpa.ai Federated Learning framework that is built upon an
holistic view of federated learning and differential privacy. It results from
the study of how to adapt the machine learning paradigm to federated learning,
and the definition of methodological guidelines for developing artificial
intelligence services based on federated learning and differential privacy. We
show how to follow the methodological guidelines with the Sherpa.ai Federated
Learning framework by means of a classification and a regression use cases.

===========================================================================
Query id: 7
I want to build a model that can predict the intensity of a tropical cyclone approaching a coastal city in real-time.
This will allow more time for preparation and evacuation.
Because there are so many variables and physical processes involved in the formation of cyclones, I want my model to take into account the complex environmental processes and take into account how these complexities would affect the confidence interval of the final predictions.
My current idea is to use a machine learning algorithm that can serve as an estimator for conditional distribution.
The model should not use any blackbox model, since I will need to be able to interpret the final prediction.
Afterwards, the predicted behaviors of cyclones must be analyzed to provide extra insight into how cyclones are formed and how they grow in intensity.
I want to look more into machine learning algorithms with proposed sampling approximations.
Finally, I will gather relevant data and information from tropical cyclones in the Atlantic region.

Best Candidate ID: 126641, Best Candidate Averaged Relevance Score: 1.444

Best Candidate:
Forecasting the weather is an increasingly data intensive exercise. Numerical
Weather Prediction (NWP) models are becoming more complex, with higher
resolutions, and there are increasing numbers of different models in operation.
While the forecasting skill of NWP models continues to improve, the number and
complexity of these models poses a new challenge for the operational
meteorologist: how should the information from all available models, each with
their own unique biases and limitations, be combined in order to provide
stakeholders with well-calibrated probabilistic forecasts to use in decision
making? In this paper, we use a road surface temperature example to demonstrate
a three-stage framework that uses machine learning to bridge the gap between
sets of separate forecasts from NWP models and the 'ideal' forecast for
decision support: probabilities of future weather outcomes. First, we use
Quantile Regression Forests to learn the error profile of each numerical model,
and use these to apply empirically-derived probability distributions to
forecasts. Second, we combine these probabilistic forecasts using quantile
averaging. Third, we interpolate between the aggregate quantiles in order to
generate a full predictive distribution, which we demonstrate has properties
suitable for decision support. Our results suggest that this approach provides
an effective and operationally viable framework for the cohesive
post-processing of weather forecasts across multiple models and lead times to
produce a well-calibrated probabilistic output.

===========================================================================
Query id: 29
I find that neural networks that take the real-time video streams from surveillance cameras usually require large memory bandwidth.
Thus, I require an iterative pruning algorithm that can compress the input images or videos while not compromising the quality of the output of the neural networks.
I hope the pruning algorithm has an adjustable magnitude of compression so that I can compress the input under different ratios based on the task.
I also want another algorithm that can reduce memory bandwidth requirements while maintaining or even improving the image quality by altering the way to pass the image into the model.
With these models, I can test their performance under various situations and then find out the critical features I should consider in these tasks.

Best Candidate ID: 191262, Best Candidate Averaged Relevance Score: 1.444

Best Candidate:
Although deep convolutional neural networks (CNNs) have achieved great
success in computer vision tasks, its real-world application is still impeded
by its voracious demand of computational resources. Current works mostly seek
to compress the network by reducing its parameters or parameter-incurred
computation, neglecting the influence of the input image on the system
complexity. Based on the fact that input images of a CNN contain substantial
redundancy, in this paper, we propose a unified framework, dubbed as ThumbNet,
to simultaneously accelerate and compress CNN models by enabling them to infer
on one thumbnail image. We provide three effective strategies to train
ThumbNet. In doing so, ThumbNet learns an inference network that performs
equally well on small images as the original-input network on large images.
With ThumbNet, not only do we obtain the thumbnail-input inference network that
can drastically reduce computation and memory requirements, but also we obtain
an image downscaler that can generate thumbnail images for generic
classification tasks. Extensive experiments show the effectiveness of ThumbNet,
and demonstrate that the thumbnail-input inference network learned by ThumbNet
can adequately retain the accuracy of the original-input network even when the
input images are downscaled 16 times.

===========================================================================
Query id: 4
I want to train a model that can predict medical event occurrence and the survival rates over time so that I can have a better analysis of the patients’ condition in a clinical trial.
My plan is to combine various deep learning models for conditional probability prediction into a single model.
I am looking for techniques from clinical research or information systems so that my model does not rely on prior knowledge of whether a medical event is going to happen or not.
Additionally, I am looking for models that are able to analyze sequential patterns and utilize time-based statistics and thus can calculate accurate event probability.
I also need strategies to make my model able to fit various sophisticated data distributions and perform well in multiple tasks in various fields.

Best Candidate ID: 291263, Best Candidate Averaged Relevance Score: 1.438

Best Candidate:
Medical practitioners use survival models to explore and understand the
relationships between patients' covariates (e.g. clinical and genetic features)
and the effectiveness of various treatment options. Standard survival models
like the linear Cox proportional hazards model require extensive feature
engineering or prior medical knowledge to model treatment interaction at an
individual level. While nonlinear survival methods, such as neural networks and
survival forests, can inherently model these high-level interaction terms, they
have yet to be shown as effective treatment recommender systems. We introduce
DeepSurv, a Cox proportional hazards deep neural network and state-of-the-art
survival method for modeling interactions between a patient's covariates and
treatment effectiveness in order to provide personalized treatment
recommendations. We perform a number of experiments training DeepSurv on
simulated and real survival data. We demonstrate that DeepSurv performs as well
as or better than other state-of-the-art survival models and validate that
DeepSurv successfully models increasingly complex relationships between a
patient's covariates and their risk of failure. We then show how DeepSurv
models the relationship between a patient's features and effectiveness of
different treatment options to show how DeepSurv can be used to provide
individual treatment recommendations. Finally, we train DeepSurv on real
clinical studies to demonstrate how it's personalized treatment recommendations
would increase the survival time of a set of patients. The predictive and
modeling capabilities of DeepSurv will enable medical researchers to use deep
neural networks as a tool in their exploration, understanding, and prediction
of the effects of a patient's characteristics on their risk of failure.

===========================================================================
Query id: 13
I am working on a problem where I need to efficiently and accurately identify relevant documents even when only a few words from a query are present.
I want to propose a methodology that can be deployed in real life, so it has to be well-documented and easy to understand for developers.
My current idea is to use semantic analysis and latent Dirichlet allocation.
I also need to develop a similarity measure and this measure has to be scalable to handle large datasets.
Additionally, I want my method to be able to identify related words in different languages and different contexts.
A sample dataset to test my method is a genomic dataset, where I need to rank search results for relevant data.
To evaluate my ranking function, I could use mean average precision as an evaluation metric.

Best Candidate ID: 160742, Best Candidate Averaged Relevance Score: 1.438

Best Candidate:
The inclusion of semantic information in any similarity measures improves the
efficiency of the similarity measure and provides human interpretable results
for further analysis. The similarity calculation method that focuses on
features related to the text's words only, will give less accurate results.
This paper presents three different methods that not only focus on the text's
words but also incorporates semantic information of texts in their feature
vector and computes semantic similarities. These methods are based on
corpus-based and knowledge-based methods, which are: cosine similarity using
tf-idf vectors, cosine similarity using word embedding and soft cosine
similarity using word embedding. Among these three, cosine similarity using
tf-idf vectors performed best in finding similarities between short news texts.
The similar texts given by the method are easy to interpret and can be used
directly in other information retrieval applications.

===========================================================================
Query id: 22
I am a medical researcher working towards Melanoma Detection.
My dataset consists of patients’ skin images.
I want to build a system that can automatically find the regions where melanoma is likely to occur.
Skin lesions are usually places where melanoma would occur, but most areas of skin lesions do not cause melanoma, so I really need to pinpoint the exact locations where melanoma could occur.
Another difficulty is that I don’t have many labeled images that show exactly where the melanoma occurs.
Most images only have a binary label.
How do I first find all the skin lesions and then pinpoint the location of the melanoma without having finely annotated data?
My current thinking is to use clustering for finding skin lesions and use some segmentation technique to find melanoma location.
Finally, the model needs to be lightweight and not use too many parameters.

Best Candidate ID: 249038, Best Candidate Averaged Relevance Score: 1.429

Best Candidate:
Melanoma is clinically difficult to distinguish from common benign skin
lesions, particularly melanocytic naevus and seborrhoeic keratosis. The
dermoscopic appearance of these lesions has huge intra-class variations and
high inter-class visual similarities. Most current research is focusing on
single-class segmentation irrespective of classes of skin lesions. In this
work, we evaluate the performance of deep learning on multi-class segmentation
of ISIC-2017 challenge dataset, which consists of 2,750 dermoscopic images. We
propose an end-to-end solution using fully convolutional networks (FCNs) for
multi-class semantic segmentation to automatically segment the melanoma,
seborrhoeic keratosis and naevus. To improve the performance of FCNs, transfer
learning and a hybrid loss function are used. We evaluate the performance of
the deep learning segmentation methods for multi-class segmentation and lesion
diagnosis (with post-processing method) on the testing set of the ISIC-2017
challenge dataset. The results showed that the two-tier level transfer learning
FCN-8s achieved the overall best result with \textit{Dice} score of 78.5% in a
naevus category, 65.3% in melanoma, and 55.7% in seborrhoeic keratosis in
multi-class segmentation and Accuracy of 84.62% for recognition of melanoma in
lesion diagnosis.

===========================================================================
Query id: 27
In law enforcement agencies, we need to quickly search through surveillance footage and retrieve videos based on text description or instructions.
Usually a lot of people are required to do the job, but now I want to develop a model that can retrieve fine-grained video of actions from videos and match these actions to the text description or instruction.
I want to find a model that can embed both video and text description of the actions into a shared embedding space, because then it would be much easier to find out the video of actions based on text description.
Also, I hope the model is trained on multiple perspectives for the same action so that the embedding is robust enough.
I might also need some techniques to improve the accuracy of cross-modal retrieval if the system’s initial accuracy is not good enough.

Best Candidate ID: 114022, Best Candidate Averaged Relevance Score: 1.429

Best Candidate:
The rapid growth of user-generated videos on the Internet has intensified the
need for text-based video retrieval systems. Traditional methods mainly favor
the concept-based paradigm on retrieval with simple queries, which are usually
ineffective for complex queries that carry far more complex semantics.
Recently, embedding-based paradigm has emerged as a popular approach. It aims
to map the queries and videos into a shared embedding space where
semantically-similar texts and videos are much closer to each other. Despite
its simplicity, it forgoes the exploitation of the syntactic structure of text
queries, making it suboptimal to model the complex queries.
  To facilitate video retrieval with complex queries, we propose a
Tree-augmented Cross-modal Encoding method by jointly learning the linguistic
structure of queries and the temporal representation of videos. Specifically,
given a complex user query, we first recursively compose a latent semantic tree
to structurally describe the text query. We then design a tree-augmented query
encoder to derive structure-aware query representation and a temporal attentive
video encoder to model the temporal characteristics of videos. Finally, both
the query and videos are mapped into a joint embedding space for matching and
ranking. In this approach, we have a better understanding and modeling of the
complex queries, thereby achieving a better video retrieval performance.
Extensive experiments on large scale video retrieval benchmark datasets
demonstrate the effectiveness of our approach.

===========================================================================
Query id: 86
I am seeking to develop a method that yields a range of promising output candidates rather than a single solution, a concept known as conformal prediction, in scenarios where data availability for the target task is limited.
I need a model capable of automatically analyzing task features and identifying similar tasks with sufficiently available training data.
This model should then discern the differences between tasks and transfer knowledge from similar tasks to new ones.
It should also retain the candidate that the user ultimately selects, storing this data under the new task for future reference.
I am currently undecided on whether to use a transformer or a convolutional neural network (CNN) for this model's implementation, so I would appreciate a comparison of these two structures.
Additionally, I would like examples demonstrating fields where conformal prediction outperforms the conventional best-one prediction.

Best Candidate ID: 2046, Best Candidate Averaged Relevance Score: 1.429

Best Candidate:
Existing research on continual learning of a sequence of tasks focused on
dealing with catastrophic forgetting, where the tasks are assumed to be
dissimilar and have little shared knowledge. Some work has also been done to
transfer previously learned knowledge to the new task when the tasks are
similar and have shared knowledge. To the best of our knowledge, no technique
has been proposed to learn a sequence of mixed similar and dissimilar tasks
that can deal with forgetting and also transfer knowledge forward and backward.
This paper proposes such a technique to learn both types of tasks in the same
network. For dissimilar tasks, the algorithm focuses on dealing with
forgetting, and for similar tasks, the algorithm focuses on selectively
transferring the knowledge learned from some similar previous tasks to improve
the new task learning. Additionally, the algorithm automatically detects
whether a new task is similar to any previous tasks. Empirical evaluation using
sequences of mixed tasks demonstrates the effectiveness of the proposed model.

===========================================================================
Query id: 5
I want to improve the interpretability and explainability of machine learning models in various fields such as healthcare and finance by using representation learning.
Since the learned representations could be highly related in certain scenarios, I want to disentangle these representations so that they are no longer related, and we can clearly see the aspects or features they are representing.
I need to first identify different metrics for representation learning, as well as some existing symmetry-based representation disentanglement techniques.
Furthermore, I want to build this procedure under a mathematical framework so that it is precise and has theoretical guarantees.
For the sake of simplicity, this procedure should induce a simple linear representation.
I also need a way to check whether existing models have the ability to induce linear disentangled representation.

Best Candidate ID: 190015, Best Candidate Averaged Relevance Score: 1.421

Best Candidate:
Learning Interpretable representation in medical applications is becoming
essential for adopting data-driven models into clinical practice. It has been
recently shown that learning a disentangled feature representation is important
for a more compact and explainable representation of the data. In this paper,
we introduce a novel adversarial variational autoencoder with a total
correlation constraint to enforce independence on the latent representation
while preserving the reconstruction fidelity. Our proposed method is validated
on a publicly available dataset showing that the learned disentangled
representation is not only interpretable, but also superior to the
state-of-the-art methods. We report a relative improvement of 81.50% in terms
of disentanglement, 11.60% in clustering, and 2% in supervised classification
with a few amounts of labeled data.

===========================================================================
Query id: 28
In archaeology, a lot of old photos are produced by polarization cameras, and I am trying to reconstruct an accurate 3D model of an object using images from these polarization cameras.
However, I have difficulties determining the surface of the object because of the features of the polarization camera.
I need a model that can analyze the angle of polarization of reflected light in captured images.
I think both geometric and photometric cues extracted from input color polarization images should be utilized to complete the analysis.
I also need a model that can optimize photometric rendering errors and handle different materials given an initial standard geometric reconstruction.
I will test the models on synthetic and real data and use them as an inspiration for my model.

Best Candidate ID: 55913, Best Candidate Averaged Relevance Score: 1.417

Best Candidate:
Reconstructing an object's high-quality 3D shape with inherent spectral
reflectance property, beyond typical device-dependent RGB albedos, opens the
door to applications requiring a high-fidelity 3D model in terms of both
geometry and photometry. In this paper, we propose a novel Multi-View Inverse
Rendering (MVIR) method called Spectral MVIR for jointly reconstructing the 3D
shape and the spectral reflectance for each point of object surfaces from
multi-view images captured using a standard RGB camera and low-cost lighting
equipment such as an LED bulb or an LED projector. Our main contributions are
twofold: (i) We present a rendering model that considers both geometric and
photometric principles in the image formation by explicitly considering camera
spectral sensitivity, light's spectral power distribution, and light source
positions. (ii) Based on the derived model, we build a cost-optimization MVIR
framework for the joint reconstruction of the 3D shape and the per-vertex
spectral reflectance while estimating the light source positions and the
shadows. Different from most existing spectral-3D acquisition methods, our
method does not require expensive special equipment and cumbersome geometric
calibration. Experimental results using both synthetic and real-world data
demonstrate that our Spectral MVIR can acquire a high-quality 3D model with
accurate spectral reflectance property.

===========================================================================
Query id: 59
Deep learning models are notoriously challenging to interpret due to their inherent black-box nature.
In my project, I aim to demystify these models by developing alternative, more transparent models that maintain high performance.
My current approach involves simplifying the network and reducing its parameters as much as possible without compromising performance.
Once the network is sufficiently simplified, we can break it down into more interpretable components.
Each component should be simple enough to visualize, and by visualizing the interaction of each component, we can gain a clearer understanding of the entire mechanism and decision process of the black-box model.
Consequently, I plan to develop a visualization toolkit to illustrate each component of the simplified neural network model.
I will also apply my proposed methodology and toolkit to various real-world scenarios where transparent decision-making is crucial, such as in credit score assessment and loan approval processes.

Best Candidate ID: 31390, Best Candidate Averaged Relevance Score: 1.417

Best Candidate:
Deep neural network (DNN) models have achieved phenomenal success for
applications in many domains, ranging from academic research in science and
engineering to industry and business. The modeling power of DNN is believed to
have come from the complexity and over-parameterization of the model, which on
the other hand has been criticized for the lack of interpretation. Although
certainly not true for every application, in some applications, especially in
economics, social science, healthcare industry, and administrative decision
making, scientists or practitioners are resistant to use predictions made by a
black-box system for multiple reasons. One reason is that a major purpose of a
study can be to make discoveries based upon the prediction function, e.g., to
reveal the relationships between measurements. Another reason can be that the
training dataset is not large enough to make researchers feel completely sure
about a purely data-driven result. Being able to examine and interpret the
prediction function will enable researchers to connect the result with existing
knowledge or gain insights about new directions to explore. Although classic
statistical models are much more explainable, their accuracy often falls
considerably below DNN. In this paper, we propose an approach to fill the gap
between relatively simple explainable models and DNN such that we can more
flexibly tune the trade-off between interpretability and accuracy. Our main
idea is a mixture of discriminative models that is trained with the guidance
from a DNN. Although mixtures of discriminative models have been studied
before, our way of generating the mixture is quite different.

===========================================================================
Query id: 71
I am currently working on a project that involves recognizing text in images, utilizing the attention-based encoder-decoder framework.
However, I am encountering difficulties when the system misinterprets words with extra or missing characters in the text, which complicates the training process and reduces accuracy.
I am in search of a model capable of assigning probabilities to words within the images, specifically regarding their likelihood of containing missing or superfluous characters.
Additionally, I require techniques that can be integrated into the existing framework.
Ideally, these techniques would enable the framework to identify potential spelling errors and attempt to restore the original, correct word.
I anticipate that the implementation of such a model and techniques will enhance the performance of my text recognition framework.
To test this, I am also seeking datasets that contain spelling errors.
I would appreciate any guidance on how to effectively train the model with these datasets and how to evaluate the models’ performance in a reliable way.

Best Candidate ID: 23761, Best Candidate Averaged Relevance Score: 1.417

Best Candidate:
Scanned documents in electronic health records (EHR) have been a challenge
for decades, and are expected to stay in the foreseeable future. Current
approaches for processing often include image preprocessing, optical character
recognition (OCR), and text mining. However, there is limited work that
evaluates the choice of image preprocessing methods, the selection of NLP
models, and the role of document layout. The impact of each element remains
unknown. We evaluated this method on a use case of two key indicators for sleep
apnea, Apnea hypopnea index (AHI) and oxygen saturation (SaO2) values, from
scanned sleep study reports. Our data that included 955 manually annotated
reports was secondarily utilized from a previous study in the University of
Texas Medical Branch. We performed image preprocessing: gray-scaling followed
by 1 iteration of dilating and erode, and 20% contrast increasing. The OCR was
implemented with the Tesseract OCR engine. A total of seven Bag-of-Words models
(Logistic Regression, Ridge Regression, Lasso Regression, Support Vector
Machine, k-Nearest Neighbor, Na\"ive Bayes, and Random Forest) and three deep
learning-based models (BiLSTM, BERT, and Clinical BERT) were evaluated. We also
evaluated the combinations of image preprocessing methods (gray-scaling, dilate
& erode, increased contrast by 20%, increased contrast by 60%), and two deep
learning architectures (with and without structured input that provides
document layout information). Our proposed method using Clinical BERT reached
an AUROC of 0.9743 and document accuracy of 94.76% for AHI, and an AUROC of
0.9523, and document accuracy of 91.61% for SaO2. We demonstrated the proper
use of image preprocessing and document layout could be beneficial to scanned
document processing.

===========================================================================
Query id: 11
I work in the field of medical text mining and I require a system that can accurately extract relevant information from medical documents.
First of all, the system needs a named entity recognition module that is capable of recognizing biomedical entities.
I am currently thinking about using deep learning based approaches for the named entity recognition module, so I can achieve optimal performance.
However, since human annotations are expensive and difficult to obtain, I want to explore options where my deep learning module does not need much labeled data, instead, it could train on unlabeled data as well.
Since in real-life, there is a large volume of medical documents, I want to reduce the amount of training time.
Currently, I think I could use some weight sharing or weight transfer to reduce training time.
My final goal is to beat current state of the art models as well as pre-trained models.
Overall, efficiency and low-cost are two important factors in my data collection, model training and inference pipeline.

Best Candidate ID: 172372, Best Candidate Averaged Relevance Score: 1.412

Best Candidate:
Developing high-performance entity normalization algorithms that can
alleviate the term variation problem is of great interest to the biomedical
community. Although deep learning-based methods have been successfully applied
to biomedical entity normalization, they often depend on traditional
context-independent word embeddings. Bidirectional Encoder Representations from
Transformers (BERT), BERT for Biomedical Text Mining (BioBERT) and BERT for
Clinical Text Mining (ClinicalBERT) were recently introduced to pre-train
contextualized word representation models using bidirectional Transformers,
advancing the state-of-the-art for many natural language processing tasks. In
this study, we proposed an entity normalization architecture by fine-tuning the
pre-trained BERT / BioBERT / ClinicalBERT models and conducted extensive
experiments to evaluate the effectiveness of the pre-trained models for
biomedical entity normalization using three different types of datasets. Our
experimental results show that the best fine-tuned models consistently
outperformed previous methods and advanced the state-of-the-art for biomedical
entity normalization, with up to 1.17% increase in accuracy.

===========================================================================
Query id: 0
I need to create a dataset for a natural language processing task and the dataset needs to be labeled.
Since the labeling process is time-consuming and very costly, before I actually create the dataset, I need to evaluate the expected quality of my data labeling process by conducting a feasibility study.
If the labeling process introduces unwanted randomness or noise, I need a method to clean these labels.
Since we will be training machine learning models on this dataset, I also need a way to estimate the expected performances of these machine learning models, possibly using the expected quality of my dataset as an indicator.
In addition, I need to estimate how much these processes cost.
Currently I am thinking about building these processes into a software system that machine learning engineers and data scientists can use.
Overall, I need to conduct several feasibility studies to identify potential challenges and make a decision about whether or not to proceed.

Best Candidate ID: 13112, Best Candidate Averaged Relevance Score: 1.4

Best Candidate:
Message passing is the core of most graph models such as Graph Convolutional
Network (GCN) and Label Propagation (LP), which usually require a large number
of clean labeled data to smooth out the neighborhood over the graph. However,
the labeling process can be tedious, costly, and error-prone in practice. In
this paper, we propose to unify active learning (AL) and message passing
towards minimizing labeling costs, e.g., making use of few and unreliable
labels that can be obtained cheaply. We make two contributions towards that
end. First, we open up a perspective by drawing a connection between AL
enforcing message passing and social influence maximization, ensuring that the
selected samples effectively improve the model performance. Second, we propose
an extension to the influence model that incorporates an explicit quality
factor to model label noise. In this way, we derive a fundamentally new AL
selection criterion for GCN and LP--reliable influence maximization (RIM)--by
considering quantity and quality of influence simultaneously. Empirical studies
on public datasets show that RIM significantly outperforms current AL methods
in terms of accuracy and efficiency.

===========================================================================
Query id: 78
I am currently working on a project that involves transforming black-and-white videos into colored versions, with a particular focus on color propagation in videos.
I have found that traditional methods of color propagation tend to be computationally demanding.
Therefore, I am seeking a more efficient method, especially when additional video information is available.
I am interested in a model that can utilize a set of colored key frames from the video and apply color propagation algorithms to colorize other frames based on the provided colored ones.
Ideally, this method should incorporate both local and global strategies.
The local strategies should ensure color consistency in neighboring regions and frames, thereby maintaining stable color propagation.
Simultaneously, from a global perspective, the model should be capable of identifying the keyframes that require colorization, ensuring overall process efficiency.
While I value the efficiency of the color propagation methods, I am willing to compromise on quality, provided the performance surpasses the baselines established within the past five years.

Best Candidate ID: 234473, Best Candidate Averaged Relevance Score: 1.4

Best Candidate:
Videos contain highly redundant information between frames. Such redundancy
has been extensively studied in video compression and encoding, but is less
explored for more advanced video processing. In this paper, we propose a
learnable unified framework for propagating a variety of visual properties of
video images, including but not limited to color, high dynamic range (HDR), and
segmentation information, where the properties are available for only a few
key-frames. Our approach is based on a temporal propagation network (TPN),
which models the transition-related affinity between a pair of frames in a
purely data-driven manner. We theoretically prove two essential factors for
TPN: (a) by regularizing the global transformation matrix as orthogonal, the
"style energy" of the property can be well preserved during propagation; (b)
such regularization can be achieved by the proposed switchable TPN with
bi-directional training on pairs of frames. We apply the switchable TPN to
three tasks: colorizing a gray-scale video based on a few color key-frames,
generating an HDR video from a low dynamic range (LDR) video and a few HDR
frames, and propagating a segmentation mask from the first frame in videos.
Experimental results show that our approach is significantly more accurate and
efficient than the state-of-the-art methods.

===========================================================================
Query id: 52
As a data analyst working with high-dimensional time series data, I am seeking to develop a model capable of handling large datasets, even those with missing values.
The model should be able to process data online efficiently, representing high-dimensional data using low-dimensional embeddings without any loss of information.
My goal is to identify a method that deconstructs embeddings of large data into simpler components, thereby facilitating efficient learning while simultaneously adjusting to new information online.
Subsequently, I aim to train a model that forecasts variables' values based on their previous embeddings.
I am particularly interested in testing this method on large, real-world datasets to evaluate its prediction accuracy.
Ultimately, my objective is to utilize this model for predicting trends in sectors such as finance, energy, and transportation, especially when dealing with complex time series data.

Best Candidate ID: 34281, Best Candidate Averaged Relevance Score: 1.389

Best Candidate:
In Internet of things (IoT), data is continuously recorded from different
data sources and devices can suffer faults in their embedded electronics, thus
leading to a high-dimensional data sets and concept drift events. Therefore,
methods that are capable of high-dimensional non-stationary time series are of
great value in IoT applications. Fuzzy Time Series (FTS) models stand out as
data-driven non-parametric models of easy implementation and high accuracy.
Unfortunately, FTS encounters difficulties when dealing with data sets of many
variables and scenarios with concept drift. We present a new approach to handle
high-dimensional non-stationary time series, by projecting the original
high-dimensional data into a low dimensional embedding space and using FTS
approach. Combining these techniques enables a better representation of the
complex content of non-stationary multivariate time series and accurate
forecasts. Our model is able to explain 98% of the variance and reach 11.52% of
RMSE, 2.68% of MAE and 2.91% of MAPE.

===========================================================================
Query id: 50
I am seeking alternatives to Generative Adversarial Networks (GANs) that can be applied to image datasets, such as CIFAR-10.
The alternative should be capable of generating new data points based on the original data distribution and should perform comparably to GANs across various metrics.
Could you provide information on the standard metrics typically used to evaluate the performance of GANs?
I anticipate that this alternative method would initially estimate and model the original data distribution, possibly using a neural network, and then generate diverse data points that adhere to the same distribution through an intelligent sampling technique.
However, I am open to learning about other promising approaches as well.

Best Candidate ID: 165335, Best Candidate Averaged Relevance Score: 1.385

Best Candidate:
This paper presents a methodology and workflow that overcome the limitations
of the conventional Generative Adversarial Networks (GANs) for geological
facies modeling. It attempts to improve the training stability and guarantee
the diversity of the generated geology through interpretable latent vectors.
The resulting samples are ensured to have the equal probability (or an unbiased
distribution) as from the training dataset. This is critical when applying GANs
to generate unbiased and representative geological models that can be further
used to facilitate objective uncertainty evaluation and optimal decision-making
in oil field exploration and development.
  We proposed and implemented a new variant of GANs called Info-WGAN for the
geological facies modeling that combines Information Maximizing Generative
Adversarial Network (InfoGAN) with Wasserstein distance and Gradient Penalty
(GP) for learning interpretable latent codes as well as generating stable and
unbiased distribution from the training data. Different from the original GAN
design, InfoGAN can use the training images with full, partial, or no labels to
perform disentanglement of the complex sedimentary types exhibited in the
training dataset to achieve the variety and diversity of the generated samples.
This is accomplished by adding additional categorical variables that provide
disentangled semantic representations besides the mere randomized latent vector
used in the original GANs. By such means, a regularization term is used to
maximize the mutual information between such latent categorical codes and the
generated geological facies in the loss function.
  Furthermore, the resulting unbiased sampling by Info-WGAN makes the data
conditioning much easier than the conventional GANs in geological modeling
because of the variety and diversity as well as the equal probability of the
unconditional sampling by the generator.

===========================================================================
Query id: 67
I am currently developing a chatbot capable of engaging in open-ended conversations with users.
However, I am dissatisfied with the conventional temperature-based stochastic sampling decoding procedure, as it results in derivative output that lacks creativity and diversity.
I am seeking literature that explores the issue of diversity deficiency in language model decoding procedures.
Additionally, I aim to create a decoding scheme that strikes a balance between output diversity and quality.
I am also interested in research that examines the phenomenon of high-likelihood sentences exhibiting unexpectedly low quality.
I wish to understand this occurrence and prevent it from affecting my decoding scheme.
Ultimately, my goal is to develop a model that surpasses existing decoding algorithms in terms of balancing the quality and diversity of generated responses.

Best Candidate ID: 170492, Best Candidate Averaged Relevance Score: 1.385

Best Candidate:
A conversational agent (chatbot) is a piece of software that is able to
communicate with humans using natural language. Modeling conversation is an
important task in natural language processing and artificial intelligence.
While chatbots can be used for various tasks, in general they have to
understand users' utterances and provide responses that are relevant to the
problem at hand.
  In my work, I conduct an in-depth survey of recent literature, examining over
70 publications related to chatbots published in the last 3 years. Then, I
proceed to make the argument that the very nature of the general conversation
domain demands approaches that are different from current state-of-of-the-art
architectures. Based on several examples from the literature I show why current
chatbot models fail to take into account enough priors when generating
responses and how this affects the quality of the conversation. In the case of
chatbots, these priors can be outside sources of information that the
conversation is conditioned on like the persona or mood of the conversers. In
addition to presenting the reasons behind this problem, I propose several ideas
on how it could be remedied.
  The next section focuses on adapting the very recent Transformer model to the
chatbot domain, which is currently state-of-the-art in neural machine
translation. I first present experiments with the vanilla model, using
conversations extracted from the Cornell Movie-Dialog Corpus. Secondly, I
augment the model with some of my ideas regarding the issues of encoder-decoder
architectures. More specifically, I feed additional features into the model
like mood or persona together with the raw conversation data. Finally, I
conduct a detailed analysis of how the vanilla model performs on conversational
data by comparing it to previous chatbot models and how the additional features
affect the quality of the generated responses.

===========================================================================
Query id: 79
I am currently involved in a project focused on developing a night-mode facial recognition application.
This technology is primarily used in security systems and driver assistance technologies that operate in dark environments.
Our project requires a system capable of efficiently processing and analyzing human facial features, head orientations, and eye gaze under near-infrared illumination.
To initiate this project, we are in search of a comprehensive dataset containing near-infrared videos of various subjects exhibiting different facial expressions.
This dataset should be meticulously labeled to allow us to validate and evaluate algorithms for face detection, eye detection, head pose, and eye gazing tasks in dim environments.
Additionally, it would be beneficial if the dataset included a diverse range of subjects to ensure the robustness and generalizability of the algorithms we develop based on it.
Moreover, we are also seeking recommendations on human-machine interaction in dim environments since the model needs to provide further instructions to the user.

Best Candidate ID: 42368, Best Candidate Averaged Relevance Score: 1.385

Best Candidate:
Commercial application of facial recognition demands robustness to a variety
of challenges such as illumination, occlusion, spoofing, disguise, etc.
Disguised face recognition is one of the emerging issues for access control
systems, such as security checkpoints at the borders. However, the lack of
availability of face databases with a variety of disguise addons limits the
development of academic research in the area. In this paper, we present a
multimodal disguised face dataset to facilitate the disguised face recognition
research. The presented database contains 8 facial add-ons and 7 additional
combinations of these add-ons to create a variety of disguised face images.
Each facial image is captured in visible, visible plus infrared, infrared, and
thermal spectra. Specifically, the database contains 100 subjects divided into
subset-A (30 subjects, 1 image per modality) and subset-B (70 subjects, 5 plus
images per modality). We also present baseline face detection results performed
on the proposed database to provide reference results and compare the
performance in different modalities. Qualitative and quantitative analysis is
performed to evaluate the challenging nature of disguise addons. The dataset
will be publicly available with the acceptance of the research article. The
database is available at: https://github.com/usmancheema89/SejongFaceDatabase.

===========================================================================
Query id: 15
I want to build a customer service chatbot that can understand and respond to complex inquiries from customers.
For my purposes, I assume the complexity in inquiries come from having multiple intents and slots in a single utterance.
Therefore I need a model that can detect intent and fill slots, jointly.
The model needs to have fast inference speed, and need to avoid introducing unseen data when restructuring the customer’s inquiry.
Therefore I should look into how to make my model faster during inference and avoid information leakage.
My current idea is to leverage the interaction between multiple intents and slots in an utterance.
These intents and slots are unlabelled, which makes them uncoordinated and more challenging.
Possibly I could try some form of graphical neural network.
Overall, I want to beat state of the art systems in this task in both speed and accuracy.

Best Candidate ID: 26015, Best Candidate Averaged Relevance Score: 1.375

Best Candidate:
The success of interactive dialog systems is usually associated with the
quality of the spoken language understanding (SLU) task, which mainly
identifies the corresponding dialog acts and slot values in each turn. By
treating utterances in isolation, most SLU systems often overlook the semantic
context in which a dialog act is expected. The act dependency between turns is
non-trivial and yet critical to the identification of the correct semantic
representations. Previous works with limited context awareness have exposed the
inadequacy of dealing with complexity in multiproned user intents, which are
subject to spontaneous change during turn transitions. In this work, we propose
to enhance SLU in multi-turn dialogs, employing a context-aware hierarchical
BERT fusion Network (CaBERT-SLU) to not only discern context information within
a dialog but also jointly identify multiple dialog acts and slots in each
utterance. Experimental results show that our approach reaches new
state-of-the-art (SOTA) performances in two complicated multi-turn dialogue
datasets with considerable improvements compared with previous methods, which
only consider single utterances for multiple intents and slot filling.

===========================================================================
Query id: 69
In my reinforcement learning (RL) research, I encountered a significant challenge: the inability of a policy learned in one environment to adapt or generalize to another.
I am seeking an efficient method to transfer policies between environments without the need to train a new policy from scratch each time the environment changes.
Currently, I am considering the use of natural language to describe the entire dynamics of the environment or to outline the anticipated changes in the environment.
To implement this, I require a language model capable of encoding these natural language descriptions.
The resulting encoded latent representations should aid in facilitating policy adaptation to different environments.
It appears crucial to align the text's meaning with the environment's dynamics, including transition probability and reward functions.
Ultimately, I aim to assess the effectiveness of this approach in terms of transfer and multi-task scenarios across various environments and compare it with existing models.
I am interested in any literature related to the use of textual descriptions in RL and any existing datasets suitable for evaluating my proposed methodology.

Best Candidate ID: 161143, Best Candidate Averaged Relevance Score: 1.375

Best Candidate:
Obtaining policies that can generalise to new environments in reinforcement
learning is challenging. In this work, we demonstrate that language
understanding via a reading policy learner is a promising vehicle for
generalisation to new environments. We propose a grounded policy learning
problem, Read to Fight Monsters (RTFM), in which the agent must jointly reason
over a language goal, relevant dynamics described in a document, and
environment observations. We procedurally generate environment dynamics and
corresponding language descriptions of the dynamics, such that agents must read
to understand new environment dynamics instead of memorising any particular
information. In addition, we propose txt2$\pi$, a model that captures three-way
interactions between the goal, document, and observations. On RTFM, txt2$\pi$
generalises to new environments with dynamics not seen during training via
reading. Furthermore, our model outperforms baselines such as FiLM and
language-conditioned CNNs on RTFM. Through curriculum learning, txt2$\pi$
produces policies that excel on complex RTFM tasks requiring several reasoning
and coreference steps.

===========================================================================
Query id: 96
As a researcher focusing on long text summarization, I am keen to understand the key features that contribute to a high-quality summary.
Which metrics would be most useful in assessing the quality of a summary?
I intend to explore the correlation between the quality of summarization and certain NLP features, such as word diversity, entropy, and perplexity.
I am curious to know if similar studies have been conducted previously.
Furthermore, I am interested in examining self-supervised alignment techniques that could assist in aligning the semantic content of a summary with its original text.
My plan is to develop a deep summarization model and an alignment model.
The alignment model, potentially using an attention mechanism, could help align summaries with longer texts.
The quality of this alignment could act as reward signals indicating the quality of the summaries, thereby driving further enhancements in my summarization model.
The improved summarization model could, in turn, enhance the quality of the alignment model.
This process would be iterative.

Best Candidate ID: 161027, Best Candidate Averaged Relevance Score: 1.368

Best Candidate:
A quality abstractive summary should not only copy salient source texts as
summaries but should also tend to generate new conceptual words to express
concrete details. Inspired by the popular pointer generator
sequence-to-sequence model, this paper presents a concept pointer network for
improving these aspects of abstractive summarization. The network leverages
knowledge-based, context-aware conceptualizations to derive an extended set of
candidate concepts. The model then points to the most appropriate choice using
both the concept set and original source text. This joint approach generates
abstractive summaries with higher-level semantic concepts. The training model
is also optimized in a way that adapts to different data, which is based on a
novel method of distantly-supervised learning guided by reference summaries and
testing set. Overall, the proposed approach provides statistically significant
improvements over several state-of-the-art models on both the DUC-2004 and
Gigaword datasets. A human evaluation of the model's abstractive abilities also
supports the quality of the summaries produced within this framework.

===========================================================================
Query id: 80
My objective is to devise a methodology for constructing intelligent robots that mimic human-like behavior.
These robots would be capable of operating in real-world environments and executing a range of complex tasks.
Such a robotic agent would interact with its surroundings, making decisions and performing actions.
More importantly, it would continuously adapt to its environment by learning from signals and cues it receives.
I aim to create a unified framework that can integrate a large number of independent modules and models.
Each of these would perform a fundamental function of the robotic agent, such as processing visual inputs, decision-making, and robotic arm manipulation.
While these sub-models would be independent, each responsible for a different function, they would need to interact with each other to optimize the overall performance of the agent.
I currently believe that the transfer of information, such as gradient information and parameter updates, from one module to another should be swift and efficient.
I intend to demonstrate the adaptability of my robotic agent to different environments and situations effectively using various datasets.

Best Candidate ID: 137555, Best Candidate Averaged Relevance Score: 1.364

Best Candidate:
In this paper, we propose a novel task, Manipulation Question Answering
(MQA), where the robot performs manipulation actions to change the environment
in order to answer a given question. To solve this problem, a framework
consisting of a QA module and a manipulation module is proposed. For the QA
module, we adopt the method for the Visual Question Answering (VQA) task. For
the manipulation module, a Deep Q Network (DQN) model is designed to generate
manipulation actions for the robot to interact with the environment. We
consider the situation where the robot continuously manipulating objects inside
a bin until the answer to the question is found. Besides, a novel dataset that
contains a variety of object models, scenarios and corresponding
question-answer pairs is established in a simulation environment. Extensive
experiments have been conducted to validate the effectiveness of the proposed
framework.

===========================================================================
Query id: 2
I am a user marketing manager, and I am planning to train a deep learning model to analyze user engagement data and thus identify key factors that influence it.
With such a model, I can continuously analyze the data and adjust marketing strategies, recommendations and rewards programs to maintain user engagement or increase future revenue.
Also, results from the model can also allow me to treat users with personalized marketing campaigns.
Therefore, I hope the model can track and analyze the data from customers in real-time.

Best Candidate ID: 249012, Best Candidate Averaged Relevance Score: 1.357

Best Candidate:
Recommender systems take inputs from user history, use an internal ranking
algorithm to generate results and possibly optimize this ranking based on
feedback. However, often the recommender system is unaware of the actual intent
of the user and simply provides recommendations dynamically without properly
understanding the thought process of the user. An intelligent recommender
system is not only useful for the user but also for businesses which want to
learn the tendencies of their users. Finding out tendencies or intents of a
user is a difficult problem to solve.
  Keeping this in mind, we sought out to create an intelligent system which
will keep track of the user's activity on a web-application as well as
determine the intent of the user in each session. We devised a way to encode
the user's activity through the sessions. Then, we have represented the
information seen by the user in a high dimensional format which is reduced to
lower dimensions using tensor factorization techniques. The aspect of intent
awareness (or scoring) is dealt with at this stage. Finally, combining the user
activity data with the contextual information gives the recommendation score.
The final recommendations are then ranked using filtering and collaborative
recommendation techniques to show the top-k recommendations to the user. A
provision for feedback is also envisioned in the current system which informs
the model to update the various weights in the recommender system. Our overall
model aims to combine both frequency-based and context-based recommendation
systems and quantify the intent of a user to provide better recommendations.
  We ran experiments on real-world timestamped user activity data, in the
setting of recommending reports to the users of a business analytics tool and
the results are better than the baselines. We also tuned certain aspects of our
model to arrive at optimized results.

===========================================================================
Query id: 76
Our team is developing street views for a mapping application, and we need to censor any potentially sensitive text information within the images we capture.
As such, we require a model capable of detecting and interpreting multi-oriented text in various scenes.
Traditional methods are proving too slow to process the volume of images we capture for the street view.
We think the most efficient solution would be to use two independent models.
The first model should be able to extract text regions from the original image, while the second should interpret the text from the region, regardless of its orientation.
We are seeking a suitable structure of models that can identify text from images, and ideally, these structures should support parallel training due to the large training set we have for this task.
The second model should also be capable of understanding different languages and local slang.
Additionally, we would prefer models that come with commercially licensed code.

Best Candidate ID: 60412, Best Candidate Averaged Relevance Score: 1.357

Best Candidate:
Key information extraction from document images is of paramount importance in
office automation. Conventional template matching based approaches fail to
generalize well to document images of unseen templates, and are not robust
against text recognition errors. In this paper, we propose an end-to-end
Spatial Dual-Modality Graph Reasoning method (SDMG-R) to extract key
information from unstructured document images. We model document images as
dual-modality graphs, nodes of which encode both the visual and textual
features of detected text regions, and edges of which represent the spatial
relations between neighboring text regions. The key information extraction is
solved by iteratively propagating messages along graph edges and reasoning the
categories of graph nodes. In order to roundly evaluate our proposed method as
well as boost the future research, we release a new dataset named WildReceipt,
which is collected and annotated tailored for the evaluation of key information
extraction from document images of unseen templates in the wild. It contains 25
key information categories, a total of about 69000 text boxes, and is about 2
times larger than the existing public datasets. Extensive experiments validate
that all information including visual features, textual features and spatial
relations can benefit key information extraction. It has been shown that SDMG-R
can effectively extract key information from document images of unseen
templates, and obtain new state-of-the-art results on the recent popular
benchmark SROIE and our WildReceipt. Our code and dataset will be publicly
released.

===========================================================================
Query id: 17
With such a model, I can help more people to learn Frisian language.
I am trying to train a model that can translate Frisian language documents.
To do this, I need a parser that uses a corpus of words in sentences with annotation and uses a tagger to assign tags to each word.
I hope this parser will be designed specifically for the West Frisian language so a Dutch tagger may work well.
Then, I am looking for some state-of-the-art translation program that can identify the morphological and syntactic structures of the text.
I need some suggestions for my model to combine the parser and the translation program’s function and generate annotations based on the morphological and syntactic structure of the translated text.

Best Candidate ID: 140517, Best Candidate Averaged Relevance Score: 1.353

Best Candidate:
Fully data-driven, deep learning-based models are usually designed as
language-independent and have been shown to be successful for many natural
language processing tasks. However, when the studied language is low-resourced
and the amount of training data is insufficient, these models can benefit from
the integration of natural language grammar-based information. We propose two
approaches to dependency parsing especially for languages with restricted
amount of training data. Our first approach combines a state-of-the-art deep
learning-based parser with a rule-based approach and the second one
incorporates morphological information into the parser. In the rule-based
approach, the parsing decisions made by the rules are encoded and concatenated
with the vector representations of the input words as additional information to
the deep network. The morphology-based approach proposes different methods to
include the morphological structure of words into the parser network.
Experiments are conducted on the IMST-UD Treebank and the results suggest that
integration of explicit knowledge about the target language to a neural parser
through a rule-based parsing system and morphological analysis leads to more
accurate annotations and hence, increases the parsing performance in terms of
attachment scores. The proposed methods are developed for Turkish, but can be
adapted to other languages as well.

===========================================================================
Query id: 73
In our security company, we often deal with low-resolution images of faces captured from surveillance cameras, which makes it difficult to identify individuals accurately.
We are looking for a solution that can generate high-resolution face images from these low-resolution inputs, especially when the input resolution is extremely low (e.g., only several hundred pixels) and the images are captured under uncontrolled settings with large pose and illumination variations.
We hope the solution is based on a deep learning framework that can efficiently utilize global and local constraints defining a face.
It would be beneficial if the framework had the following 2-step processes: one for holistic face reconstruction according to global constraints and another for enhancing face-specific details and enforcing local patch statistics.
Additionally, we would like the deep network to be optimized using a loss function for super-resolution that combines reconstruction error with a learned face quality measured in an adversarial setting, resulting in robust visual results.
We are interested in a solution that has been extensively tested in both controlled and uncontrolled setups and has demonstrated improvements over the current state of the art.

Best Candidate ID: 165999, Best Candidate Averaged Relevance Score: 1.353

Best Candidate:
To learn disentangled representations of facial images, we present a Dual
Encoder-Decoder based Generative Adversarial Network (DED-GAN). In the proposed
method, both the generator and discriminator are designed with deep
encoder-decoder architectures as their backbones. To be more specific, the
encoder-decoder structured generator is used to learn a pose disentangled face
representation, and the encoder-decoder structured discriminator is tasked to
perform real/fake classification, face reconstruction, determining identity and
estimating face pose. We further improve the proposed network architecture by
minimising the additional pixel-wise loss defined by the Wasserstein distance
at the output of the discriminator so that the adversarial framework can be
better trained. Additionally, we consider face pose variation to be continuous,
rather than discrete in existing literature, to inject richer pose information
into our model. The pose estimation task is formulated as a regression problem,
which helps to disentangle identity information from pose variations. The
proposed network is evaluated on the tasks of pose-invariant face recognition
(PIFR) and face synthesis across poses. An extensive quantitative and
qualitative evaluation carried out on several controlled and in-the-wild
benchmarking datasets demonstrates the superiority of the proposed DED-GAN
method over the state-of-the-art approaches.

===========================================================================
Query id: 30
My cousin is struggling with math and I want to help him.
I need a system that can help people to understand arithmetic operations and how to solve algebraic equations.
The system needs to be able to generate answer rationales that can include detailed steps taken to arrive at a final answer.
It would be best if the system can provide a framework for the structure of the answer that helps my cousin to understand the standard way of solving math problems.
Also, I hope the system can adapt its answer rationales to different levels of understanding and provide feedback to the user’s progress.
I hope with the help of the system, my cousin can not only understand math but also love math.

Best Candidate ID: 94644, Best Candidate Averaged Relevance Score: 1.333

Best Candidate:
Explainable question answering systems predict an answer together with an
explanation showing why the answer has been selected. The goal is to enable
users to assess the correctness of the system and understand its reasoning
process. However, we show that current models and evaluation settings have
shortcomings regarding the coupling of answer and explanation which might cause
serious issues in user experience. As a remedy, we propose a hierarchical model
and a new regularization term to strengthen the answer-explanation coupling as
well as two evaluation scores to quantify the coupling. We conduct experiments
on the HOTPOTQA benchmark data set and perform a user study. The user study
shows that our models increase the ability of the users to judge the
correctness of the system and that scores like F1 are not enough to estimate
the usefulness of a model in a practical setting with human users. Our scores
are better aligned with user experience, making them promising candidates for
model selection.

===========================================================================
Query id: 65
I am currently working on a project aimed at developing a chatbot capable of handling open-domain Q&A for technical queries within specific domains.
To fine-tune a language model within this domain, I require a suitable dataset for domain adaptation and model fine-tuning.
My interest lies in a dataset comprising real-world questions from users on technical forums, as opposed to artificially generated queries.
The dataset should ideally be sufficiently large to train a model.
Additionally, I am seeking an auxiliary resource for pretraining and learning representations of domain-specific terminologies.
This resource could potentially be an external knowledge base, such as Wikipedia.
Any academic papers that propose or utilize such a dataset would be of great assistance to my research, as they would aid in the development of a more effective and accurate chatbot system.
Furthermore, I am interested in standard methodologies for creating and curating such a dataset.

Best Candidate ID: 16259, Best Candidate Averaged Relevance Score: 1.333

Best Candidate:
With the rise of large-scale pre-trained language models, open-domain
question-answering (ODQA) has become an important research topic in NLP. Based
on the popular pre-training fine-tuning approach, we posit that an additional
in-domain pre-training stage using a large-scale, natural, and diverse
question-answering (QA) dataset can be beneficial for ODQA. Consequently, we
propose a novel QA dataset based on the Common Crawl project in this paper.
Using the readily available schema.org annotation, we extract around 130
million multilingual question-answer pairs, including about 60 million English
data-points. With this previously unseen number of natural QA pairs, we
pre-train popular language models to show the potential of large-scale
in-domain pre-training for the task of question-answering. In our experiments,
we find that pre-training question-answering models on our Common Crawl
Question Answering dataset (CCQA) achieves promising results in zero-shot, low
resource and fine-tuned settings across multiple tasks, models and benchmarks.

===========================================================================
Query id: 83
When modeling complex probability distributions of variables with inherent dependencies, I utilize a graph to represent these dependencies and probabilistic models to depict the variables' distributions.
However, these models can be large and computationally expensive during inference.
Therefore, I am seeking a fast, approximate inference algorithm that can access a highly effective proposal distribution for importance sampling.
My current strategy involves identifying non-essential variables and marginalizing them during sampling to enhance the speed of inference.
Additionally, I am considering transforming the graphical model into a more efficient format, such as a decision diagram, to facilitate easier interpretation during sampling (inference).
To evaluate the effectiveness of my approach, I need to test it against standard benchmarks.
Therefore, I would appreciate information on the standard benchmarks applicable to this task.

Best Candidate ID: 73754, Best Candidate Averaged Relevance Score: 1.333

Best Candidate:
Sampling is a popular method for approximate inference when exact inference
is impractical. Generally, sampling algorithms do not exploit context-specific
independence (CSI) properties of probability distributions. We introduce
context-specific likelihood weighting (CS-LW), a new sampling methodology,
which besides exploiting the classical conditional independence properties,
also exploits CSI properties. Unlike the standard likelihood weighting, CS-LW
is based on partial assignments of random variables and requires fewer samples
for convergence due to the sampling variance reduction. Furthermore, the speed
of generating samples increases. Our novel notion of contextual assignments
theoretically justifies CS-LW. We empirically show that CS-LW is competitive
with state-of-the-art algorithms for approximate inference in the presence of a
significant amount of CSIs.

===========================================================================
Query id: 87
I am interested in understanding how modern computer programs can emulate human causal logic.
To achieve this, I would like to learn about the latest computational models that can predict human judgments and compare their performance with that of humans.
Additionally, I am seeking references where human participants are presented with causal structures generated by Bayesian-related models, along with their explanations or feedback based on these model-generated outputs.
I am curious about the most common differences between human decision-making and machine-based decisions, as well as the history of the development of models for causal reasoning, particularly how these models draw inspiration from the neural structures of living creatures.

Best Candidate ID: 48235, Best Candidate Averaged Relevance Score: 1.333

Best Candidate:
Research in Cognitive Science suggests that humans understand and represent
knowledge of the world through causal relationships. In addition to
observations, they can rely on experimenting and counterfactual reasoning --
i.e. referring to an alternative course of events -- to identify causal
relations and explain atypical situations. Different instances of control
systems, such as smart homes, would benefit from having a similar causal model,
as it would help the user understand the logic of the system and better react
when needed. However, while data-driven methods achieve high levels of
correlation detection, they mainly fall short of finding causal relations,
notably being limited to observations only. Notably, they struggle to identify
the cause from the effect when detecting a correlation between two variables.
This paper introduces a new way to learn causal models from a mixture of
experiments on the environment and observational data. The core of our method
is the use of selected interventions, especially our learning takes into
account the variables where it is impossible to intervene, unlike other
approaches. The causal model we obtain is then used to generate Causal Bayesian
Networks, which can be later used to perform diagnostic and predictive
inference. We use our method on a smart home simulation, a use case where
knowing causal relations pave the way towards explainable systems. Our
algorithm succeeds in generating a Causal Bayesian Network close to the
simulation's ground truth causal interactions, showing encouraging prospects
for application in real-life systems.

===========================================================================
Query id: 23
I am working on autonomous driving research and I need to minimize the risk of autonomous vehicles to the public, especially pedestrians.
Therefore, I am trying to build a model that will inform my autonomous vehicles to avoid crowded streets where lots of people are walking.
This model has to be deployed in real-time.
My current idea is to use a computer vision model that can extract and count the number of pedestrians from surveillance footage videos.
It needs to be a fast pedestrian detection model.
How should I go about doing this?
Should my model be able to process videos or simply just images ?
If I use videos, the computational load would be higher but I could get a more accurate estimate of the walking speed of these pedestrians during a time interval.

Best Candidate ID: 76685, Best Candidate Averaged Relevance Score: 1.3

Best Candidate:
Detecting and predicting the behavior of pedestrians is extremely crucial for
self-driving vehicles to plan and interact with them safely. Although there
have been several research works in this area, it is important to have fast and
memory efficient models such that it can operate in embedded hardware in these
autonomous machines. In this work, we propose a novel architecture using
spatial-temporal multi-tasking to do camera based pedestrian detection and
intention prediction. Our approach significantly reduces the latency by being
able to detect and predict all pedestrians' intention in a single shot manner
while also being able to attain better accuracy by sharing features with
relevant object level information and interactions.

===========================================================================
Query id: 37
I need a literature review of the clinician system for a project I am planning to do.
I am specifically interested in the application of reinforcement learning (RL) in the clinician system and identify the benefits of RL for the healthcare research community.
I hope the literature review can contain detailed exploration of the findings from the literature, including an explanation of the clinician system, how intensive care data can be utilized, and how reinforcement learning can help to make treatment recommendations.
I hope the literature review also compares the differences of different RL algorithms used in the clinician system field, and finds out the one that is most efficient under large amounts of data.

Best Candidate ID: 31177, Best Candidate Averaged Relevance Score: 1.273

Best Candidate:
The rapid increase in the percentage of chronic disease patients along with
the recent pandemic pose immediate threats on healthcare expenditure and
elevate causes of death. This calls for transforming healthcare systems away
from one-on-one patient treatment into intelligent health systems, to improve
services, access and scalability, while reducing costs. Reinforcement Learning
(RL) has witnessed an intrinsic breakthrough in solving a variety of complex
problems for diverse applications and services. Thus, we conduct in this paper
a comprehensive survey of the recent models and techniques of RL that have been
developed/used for supporting Intelligent-healthcare (I-health) systems. This
paper can guide the readers to deeply understand the state-of-the-art regarding
the use of RL in the context of I-health. Specifically, we first present an
overview for the I-health systems challenges, architecture, and how RL can
benefit these systems. We then review the background and mathematical modeling
of different RL, Deep RL (DRL), and multi-agent RL models. After that, we
provide a deep literature review for the applications of RL in I-health
systems. In particular, three main areas have been tackled, i.e., edge
intelligence, smart core network, and dynamic treatment regimes. Finally, we
highlight emerging challenges and outline future research directions in driving
the future success of RL in I-health systems, which opens the door for
exploring some interesting and unsolved problems.

===========================================================================
Query id: 47
I am working on designing a universal machine translation model that can adapt different learning strategies when asked to translate any two languages.
My current idea is to utilize the structure of natural data to help the model to learn a specialized tokenization, preprocessing, alignment and training strategy for a specific language.
Therefore, my model should be able to detect the inherent structure of a given language.
In terms of how the model would adapt to a specialized strategy, I want to incorporate reinforcement learning techniques here.
I might first try casting this problem as a multi-armed bandit problem.
Under this setting, I will first include a list of possible actions that the model is allowed to make.
These actions include hyperparameter choices, neural network architecture choices, tokenization choices.
I hope the structure of natural data will make sure the loss function in the multi-armed bandit problem has certain trend structure and structural properties.
Utilizing these trend structures in the loss, I hope my model can make interpretable choices in its decision making.

Best Candidate ID: 307923, Best Candidate Averaged Relevance Score: 1.273

Best Candidate:
This thesis investigates how the sub-structure of words can be accounted for
in probabilistic models of language. Such models play an important role in
natural language processing tasks such as translation or speech recognition,
but often rely on the simplistic assumption that words are opaque symbols. This
assumption does not fit morphologically complex language well, where words can
have rich internal structure and sub-word elements are shared across distinct
word forms.
  Our approach is to encode basic notions of morphology into the assumptions of
three different types of language models, with the intention that leveraging
shared sub-word structure can improve model performance and help overcome data
sparsity that arises from morphological processes.
  In the context of n-gram language modelling, we formulate a new Bayesian
model that relies on the decomposition of compound words to attain better
smoothing, and we develop a new distributed language model that learns vector
representations of morphemes and leverages them to link together
morphologically related words. In both cases, we show that accounting for word
sub-structure improves the models' intrinsic performance and provides benefits
when applied to other tasks, including machine translation.
  We then shift the focus beyond the modelling of word sequences and consider
models that automatically learn what the sub-word elements of a given language
are, given an unannotated list of words. We formulate a novel model that can
learn discontiguous morphemes in addition to the more conventional contiguous
morphemes that most previous models are limited to. This approach is
demonstrated on Semitic languages, and we find that modelling discontiguous
sub-word structures leads to improvements in the task of segmenting words into
their contiguous morphemes.

===========================================================================
Query id: 8
I want a model to produce a prediction distribution closely approximating a delta function centered at a single value, but  I also need to make sure the optimal prediction distribution is not an actual delta function.
If I use mean squared loss, the optimal distribution will tend to the delta function.
So I need to address this issue by using a different loss.
I am thinking about using generative models that can sample its output.
Maybe rejection sampling?
This problem might be related to mode collapses and how to avoid them, so I would like to explore that as well.
In addition, the model should not use too many parameters and I want to keep everything in low dimensional spaces.
Finally, to test if my model works, I want to test on a small image dataset.
In this setting, the delta function would be the salient feature or event in each image.

Best Candidate ID: 169710, Best Candidate Averaged Relevance Score: 1.267

Best Candidate:
Despite excellent progress in recent years, mode collapse remains a major
unsolved problem in generative adversarial networks (GANs).In this paper, we
present spectral regularization for GANs (SR-GANs), a new and robust method for
combating the mode collapse problem in GANs. Theoretical analysis shows that
the optimal solution to the discriminator has a strong relationship to the
spectral distributions of the weight matrix.Therefore, we monitor the spectral
distribution in the discriminator of spectral normalized GANs (SN-GANs), and
discover a phenomenon which we refer to as spectral collapse, where a large
number of singular values of the weight matrices drop dramatically when mode
collapse occurs. We show that there are strong evidence linking mode collapse
to spectral collapse; and based on this link, we set out to tackle spectral
collapse as a surrogate of mode collapse. We have developed a spectral
regularization method where we compensate the spectral distributions of the
weight matrices to prevent them from collapsing, which in turn successfully
prevents mode collapse in GANs. We provide theoretical explanations for why
SR-GANs are more stable and can provide better performances than SN-GANs. We
also present extensive experimental results and analysis to show that SR-GANs
not only always outperform SN-GANs but also always succeed in combating mode
collapse where SN-GANs fail. The code is available at
https://github.com/max-liu-112/SRGANs-Spectral-Regularization-GANs-.

===========================================================================
Query id: 24
As a family member of a border control officer, I want to make his life easier.
I require a facial recognition system that utilizes Deep Neural Networks to verify identities from images that can contain camouflaged people .
For example, some people may wear a wig or cosmetic contact lenses when taking photos at the border control.
The system must be automated and provide transparency in decision-making, presenting information in a way that is easily understood.
I need a framework to evaluate the quality of the method, including its ability to detect and remove artifacts, and analyze their influence on decision-making.
The system must be able to identify uncertain or incorrect decisions and adjust its highlighting method accordingly.
For example, when processing a large number of travelers at a busy airport, I need a reliable and efficient system that can quickly and accurately verify identities and detect any potential security threats.

Best Candidate ID: 176936, Best Candidate Averaged Relevance Score: 1.25

Best Candidate:
Face recognition has evolved as a prominent biometric authentication
modality. However, vulnerability to presentation attacks curtails its reliable
deployment. Automatic detection of presentation attacks is essential for secure
use of face recognition technology in unattended scenarios. In this work, we
introduce a Convolutional Neural Network (CNN) based framework for presentation
attack detection, with deep pixel-wise supervision. The framework uses only
frame level information making it suitable for deployment in smart devices with
minimal computational and time overhead. We demonstrate the effectiveness of
the proposed approach in public datasets for both intra as well as
cross-dataset experiments. The proposed approach achieves an HTER of 0% in
Replay Mobile dataset and an ACER of 0.42% in Protocol-1 of OULU dataset
outperforming state of the art methods.

===========================================================================
Query id: 33
I am doing theoretical experiments on whether deep learning models can understand quantitative logic reasoning.
To continue my experiment, I am looking for a deep learning model that can perform deductive and quantitative inference tasks.
Deductive tasks include several premises and a conclusion, and the model needs to identify whether the conclusion is reasonable based on the premise; and quantitative inference problem needs the model to interpret real-life problems into math problems and then solve it.
I hope the single model can handle both deductive, quantitative inference tasks and a task that is a combination of the 2 subtasks.
Additionally, I need another system that is able to efficiently take answers from humans, extract the key part and compare human answers to the answers generated by the model.

Best Candidate ID: 203039, Best Candidate Averaged Relevance Score: 1.25

Best Candidate:
Deep learning is very effective at jointly learning feature representations
and classification models, especially when dealing with high dimensional input
patterns. Probabilistic logic reasoning, on the other hand, is capable to take
consistent and robust decisions in complex environments. The integration of
deep learning and logic reasoning is still an open-research problem and it is
considered to be the key for the development of real intelligent agents. This
paper presents Deep Logic Models, which are deep graphical models integrating
deep learning and logic reasoning both for learning and inference. Deep Logic
Models create an end-to-end differentiable architecture, where deep learners
are embedded into a network implementing a continuous relaxation of the logic
knowledge. The learning process allows to jointly learn the weights of the deep
learners and the meta-parameters controlling the high-level reasoning. The
experimental results show that the proposed methodology overtakes the
limitations of the other approaches that have been proposed to bridge deep
learning and reasoning.

===========================================================================
Query id: 46
As a computer vision researcher, I need a system that does 3D face reconstruction from images from a single camera.
My current idea is to leverage the highly correlated spectral and spatial information in 3D visual face models, which are treated as tensors.
I want to introduce a tensor completion model to generate a vast amount of possible face completions from all angles and poses.
Given these generated images related to a single facial image, the system should use semantic segmentation to identify different facial parts.
From these facial parts, the system should discover geometry and global structure of the face.
In the end, the system should also determine the best subset of generated images that can be used to optimize the likelihood of the 3D facial reconstruction, conditioned on learned geometry and global structure of the face.
The system should outperform the current state-of-the-art facial reconstruction models.

Best Candidate ID: 270795, Best Candidate Averaged Relevance Score: 1.25

Best Candidate:
3D face reconstruction is a fundamental Computer Vision problem of
extraordinary difficulty. Current systems often assume the availability of
multiple facial images (sometimes from the same subject) as input, and must
address a number of methodological challenges such as establishing dense
correspondences across large facial poses, expressions, and non-uniform
illumination. In general these methods require complex and inefficient
pipelines for model building and fitting. In this work, we propose to address
many of these limitations by training a Convolutional Neural Network (CNN) on
an appropriate dataset consisting of 2D images and 3D facial models or scans.
Our CNN works with just a single 2D facial image, does not require accurate
alignment nor establishes dense correspondence between images, works for
arbitrary facial poses and expressions, and can be used to reconstruct the
whole 3D facial geometry (including the non-visible parts of the face)
bypassing the construction (during training) and fitting (during testing) of a
3D Morphable Model. We achieve this via a simple CNN architecture that performs
direct regression of a volumetric representation of the 3D facial geometry from
a single 2D image. We also demonstrate how the related task of facial landmark
localization can be incorporated into the proposed framework and help improve
reconstruction quality, especially for the cases of large poses and facial
expressions. Testing code will be made available online, along with pre-trained
models http://aaronsplace.co.uk/papers/jackson2017recon

===========================================================================
Query id: 51
I am currently working on a machine learning project in which I aim to train a model to solve a zero-sum game, such as two-player tic-tac-toe.
The trained model will be designed to compete against either the system or a human player, and it could also potentially play against another version of itself.
I want to utilize gradient descent to train my model.
Therefore, the initial task is to formulate this zero-sum game in a manner that makes it differentiable and thus suitable for the application of gradient descent.
Moreover, I am seeking to circumvent the potential pitfalls of gradient descent, specifically situations where the algorithm becomes trapped in a suboptimal local minimum and fails to locate the global optimum.
I welcome any techniques that could enhance the efficiency of my gradient descent algorithm within this context.
Ideally, my gradient learning algorithm should possess robust theoretical properties, ensuring convergence under mild conditions.

Best Candidate ID: 198036, Best Candidate Averaged Relevance Score: 1.25

Best Candidate:
Recent applications that arise in machine learning have surged significant
interest in solving min-max saddle point games. This problem has been
extensively studied in the convex-concave regime for which a global equilibrium
solution can be computed efficiently. In this paper, we study the problem in
the non-convex regime and show that an \varepsilon--first order stationary
point of the game can be computed when one of the player's objective can be
optimized to global optimality efficiently. In particular, we first consider
the case where the objective of one of the players satisfies the
Polyak-{\L}ojasiewicz (PL) condition. For such a game, we show that a simple
multi-step gradient descent-ascent algorithm finds an \varepsilon--first order
stationary point of the problem in \widetilde{\mathcal{O}}(\varepsilon^{-2})
iterations. Then we show that our framework can also be applied to the case
where the objective of the "max-player" is concave. In this case, we propose a
multi-step gradient descent-ascent algorithm that finds an \varepsilon--first
order stationary point of the game in \widetilde{\cal O}(\varepsilon^{-3.5})
iterations, which is the best known rate in the literature. We applied our
algorithm to a fair classification problem of Fashion-MNIST dataset and
observed that the proposed algorithm results in smoother training and better
generalization.

===========================================================================
Query id: 38
I am interested in models that work as intelligent personal assistants that can accurately understand and execute user commands.
To achieve this, I am expecting a model that has 2 ways of understanding human commands.
First, I hope the model needs to understand the high-level intention of the list of commands.
Then, the model also needs some technique to separate the list of commands into simple actions it can take and validate whether each action is reasonable enough with the help of the intention it derived.
I heard the Feudal Reinforcement Learning model can help, but other models may also help.
Additionally, I hope the model can ask the user whether they need to change the commands if it finds some of the commands are bizarre.
I already have a large enough dataset about whether a command is reasonable under certain intentions, so it is okay if the model is not trained.

Best Candidate ID: 247327, Best Candidate Averaged Relevance Score: 1.231

Best Candidate:
As more robots act in physical proximity to people, it is essential to ensure
they make decisions and execute actions that align with human values. To do so,
robots need to understand the true intentions behind human-issued commands. In
this paper, we define a safe robot as one that receives a natural-language
command from humans, considers an action in response to that command, and
accurately predicts how humans will judge that action if is executed in
reality. Our contribution is two-fold: First, we introduce a web platform for
human users to propose commands to simulated robots. The robots receive
commands and act based on those proposed commands, and then the users provide
positive and/or negative reinforcement. Next, we train a critic for each robot
to predict the crowd's responses to one of the crowd-proposed commands. Second,
we show that the morphology of a robot plays a role in the way it grounds
language: The critics show that two of the robots used in the experiment
achieve a lower prediction error than the others. Thus, those two robots are
safer, according to our definition, since they ground the proposed command more
accurately.

===========================================================================
Query id: 95
I aim to develop a comprehensive fake news detection system capable of identifying and curbing the spread of misinformation.
The system should be adaptable enough to detect fake news across various domains, such as tracking misinformation related to COVID-19 on social media platforms.
Given the multimedia nature of news, the system should incorporate an automatic speech recognition module, an image (video) processing module, and a language model, among other potential components.
It would be beneficial to convert audio recognition into transcripts and image and video processing into natural language descriptions, necessitating the design of a robust language model.
Additionally, I am keen to explore the dynamics of large-scale information dissemination, which I hope to address by my proposed fake news detection system.

Best Candidate ID: 136352, Best Candidate Averaged Relevance Score: 1.231

Best Candidate:
Dense video captioning is a task of localizing interesting events from an
untrimmed video and producing textual description (captions) for each localized
event. Most of the previous works in dense video captioning are solely based on
visual information and completely ignore the audio track. However, audio, and
speech, in particular, are vital cues for a human observer in understanding an
environment. In this paper, we present a new dense video captioning approach
that is able to utilize any number of modalities for event description.
Specifically, we show how audio and speech modalities may improve a dense video
captioning model. We apply automatic speech recognition (ASR) system to obtain
a temporally aligned textual description of the speech (similar to subtitles)
and treat it as a separate input alongside video frames and the corresponding
audio track. We formulate the captioning task as a machine translation problem
and utilize recently proposed Transformer architecture to convert multi-modal
input data into textual descriptions. We demonstrate the performance of our
model on ActivityNet Captions dataset. The ablation studies indicate a
considerable contribution from audio and speech components suggesting that
these modalities contain substantial complementary information to video frames.
Furthermore, we provide an in-depth analysis of the ActivityNet Caption results
by leveraging the category tags obtained from original YouTube videos. Code is
publicly available: github.com/v-iashin/MDVC

===========================================================================
Query id: 99
In the context of time series forecasting, it's observed that anomalies in certain data sources, such as seismic activity, heart rate monitors, and nuclear plant cooling system temperatures, occur infrequently but often result in catastrophic outcomes like earthquakes, heart attacks, and nuclear meltdowns.
My objective is to devise a time series forecasting model capable of predicting these rare anomalies with high precision.
This model must be equipped to manage extremely sparse supervised signals.
To train my model effectively, I believe it's necessary to employ some form of sampling or bootstrapping to simulate various event pathways leading to these anomalies.
Essentially, this involves data augmentation to generate more positive signals for training.
Consequently, I require a simulation agent capable of learning the underlying time series data distribution.
This simulation agent (model) must also be generative to create new pathways and simulate new events.
I propose that the simulation agent should incorporate a reinforcement learning component.
However, I am open to any literature that discusses rare event simulation, regardless of whether the method is statistical or deep learning-based.
Ultimately, I am confident that my model will be robust enough for deployment in real-world scenarios for disaster prevention and early warning systems.

Best Candidate ID: 49258, Best Candidate Averaged Relevance Score: 1.231

Best Candidate:
One of the limiting factors in training data-driven, rare-event prediction
algorithms is the scarcity of the events of interest resulting in an extreme
imbalance in the data. There have been many methods introduced in the
literature for overcoming this issue; simple data manipulation through
undersampling and oversampling, utilizing cost-sensitive learning algorithms,
or by generating synthetic data points following the distribution of the
existing data. While synthetic data generation has recently received a great
deal of attention, there are real challenges involved in doing so for
high-dimensional data such as multivariate time series. In this study, we
explore the usefulness of the conditional generative adversarial network (CGAN)
as a means to perform data-informed oversampling in order to balance a large
dataset of multivariate time series. We utilize a flare forecasting benchmark
dataset, named SWAN-SF, and design two verification methods to both
quantitatively and qualitatively evaluate the similarity between the generated
minority and the ground-truth samples. We further assess the quality of the
generated samples by training a classical, supervised machine learning
algorithm on synthetic data, and testing the trained model on the unseen, real
data. The results show that the classifier trained on the data augmented with
the synthetic multivariate time series achieves a significant improvement
compared with the case where no augmentation is used. The popular flare
forecasting evaluation metrics, TSS and HSS, report 20-fold and 5-fold
improvements, respectively, indicating the remarkable statistical similarities,
and the usefulness of CGAN-based data generation for complicated tasks such as
flare forecasting.

===========================================================================
Query id: 16
I am working on a project where I input a sentence, my system will return related images based on my input sentence.
I call this task image retrieval from sentences.
This system needs to work with different languages.
My current plan is to first create a synthetic dataset that consists of tuples of sentences in different languages and an image.
I am trying to use image captioning or image annotation pre-trained models to create this synthetic dataset.
After we have created this synthetic dataset, I will build a model that can learn the representations of sentences with respect to the image.
I believe my work is in the intersection of information retrieval, learning sentence representation and data augmentation.
In the end, I want my work to be integrated with a search engine that can be deployed in the cloud.

Best Candidate ID: 306611, Best Candidate Averaged Relevance Score: 1.222

Best Candidate:
The ability to describe images with natural language sentences is the
hallmark for image and language understanding. Such a system has wide ranging
applications such as annotating images and using natural sentences to search
for images.In this project we focus on the task of bidirectional image
retrieval: such asystem is capable of retrieving an image based on a sentence
(image search) andretrieve sentence based on an image query (image annotation).
We present asystem based on a global ranking objective function which uses a
combinationof convolutional neural networks (CNN) and multi layer perceptrons
(MLP).It takes a pair of image and sentence and processes them in different
channels,finally embedding it into a common multimodal vector space. These
embeddingsencode abstract semantic information about the two inputs and can be
comparedusing traditional information retrieval approaches. For each such pair,
the modelreturns a score which is interpretted as a similarity metric. If this
score is high,the image and sentence are likely to convey similar meaning, and
if the score is low then they are likely not to.
  The visual input is modeled via deep convolutional neural network. On
theother hand we explore three models for the textual module. The first one
isbag of words with an MLP. The second one uses n-grams (bigram, trigrams,and a
combination of trigram & skip-grams) with an MLP. The third is morespecialized
deep network specific for modeling variable length sequences (SSE).We report
comparable performance to recent work in the field, even though ouroverall
model is simpler. We also show that the training time choice of how wecan
generate our negative samples has a significant impact on performance, and can
be used to specialize the bi-directional system in one particular task.

===========================================================================
Query id: 49
My goal is to develop a deep neural network agent that can navigate in diverse, unseen environments where changes in objects, textures, colors, and noises introduced naturally from faulty devices and signal interferences are common.
In a straightforward manner, I could annotate a diverse set of fully labeled objects, textures and scene layouts, all of which are contaminated by noises to some extent.
Then the agent will be trained with varying levels of augmentations.
Moreover, I would like my model to be structured enough so that I could interpret the effects of introducing noises and transforms to the model’s input.
What are some architectural designs for my model?
In addition, I want my model to recognize geometric invariants in images even if they are corrupted by noises and perturbed by circumstances.
In the end, my agent needs to be robust against all these perturbations and noises.
A real-life scenario could involve developing an autonomous driving system that can navigate safely in diverse, challenging environments.

Best Candidate ID: 252993, Best Candidate Averaged Relevance Score: 1.2

Best Candidate:
Despite the appeal of deep neural networks that largely replace the
traditional handmade filters, they still suffer from isolated cases that cannot
be properly handled only by the training of convolutional filters. Abnormal
factors, including real-world noise, blur, or other quality degradations, ruin
the output of a neural network. These unexpected problems can produce critical
complications, and it is surprising that there has only been minimal research
into the effects of noise in the deep neural network model. Therefore, we
present an exhaustive investigation into the effect of noise in image
classification and suggest a generalized architecture of a dual-channel model
to treat quality degraded input images. We compare the proposed dual-channel
model with a simple single model and show it improves the overall performance
of neural networks on various types of quality degraded input datasets.

===========================================================================
Query id: 92
I aim to assist actors in better comprehending their roles in movies or TV shows by creating a system that can comprehend and analyze the mood and personality of characters within a narrative context.
My plan is to utilize transformer-based models in conjunction with a reinforcement learning (RL) framework to accomplish this.
The transformer-based model should be capable of inferring from the context and dialogue to predict a character's potential mood and anticipate their next line.
The RL framework will then compare the predicted conversation with the actual conversation in the reward formula to provide weak self-supervision during the training process.
I am aware of the concept of a warm start in training, but I am unsure how to adapt this to my current project.
Furthermore, the target narrative contexts, such as scripts, differ from typical texts, so I would appreciate advice on the appropriate length for the context window in this type of text.
Finally, the model should be able to map mood or personality to facial expressions or character appearances to provide guidance to the actors.

Best Candidate ID: 291102, Best Candidate Averaged Relevance Score: 1.2

Best Candidate:
Recent neural models of dialogue generation offer great promise for
generating responses for conversational agents, but tend to be shortsighted,
predicting utterances one at a time while ignoring their influence on future
outcomes. Modeling the future direction of a dialogue is crucial to generating
coherent, interesting dialogues, a need which led traditional NLP models of
dialogue to draw on reinforcement learning. In this paper, we show how to
integrate these goals, applying deep reinforcement learning to model future
reward in chatbot dialogue. The model simulates dialogues between two virtual
agents, using policy gradient methods to reward sequences that display three
useful conversational properties: informativity (non-repetitive turns),
coherence, and ease of answering (related to forward-looking function). We
evaluate our model on diversity, length as well as with human judges, showing
that the proposed algorithm generates more interactive responses and manages to
foster a more sustained conversation in dialogue simulation. This work marks a
first step towards learning a neural conversational model based on the
long-term success of dialogues.

===========================================================================
Query id: 1
I need to train a GAN that can generate high-quality outputs for various applications.
To do this, I need a system that is able to store and manage the massive amount of training data.
Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data.
In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work.
Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method.
With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.

Best Candidate ID: 62797, Best Candidate Averaged Relevance Score: 1.167

Best Candidate:
Training deep neural networks on large datasets containing high-dimensional
data requires a large amount of computation. A solution to this problem is
data-parallel distributed training, where a model is replicated into several
computational nodes that have access to different chunks of the data. This
approach, however, entails high communication rates and latency because of the
computed gradients that need to be shared among nodes at every iteration. The
problem becomes more pronounced in the case that there is wireless
communication between the nodes (i.e. due to the limited network bandwidth). To
address this problem, various compression methods have been proposed including
sparsification, quantization, and entropy encoding of the gradients. Existing
methods leverage the intra-node information redundancy, that is, they compress
gradients at each node independently. In contrast, we advocate that the
gradients across the nodes are correlated and propose methods to leverage this
inter-node redundancy to improve compression efficiency. Depending on the node
communication protocol (parameter server or ring-allreduce), we propose two
instances of the LGC approach that we coin Learned Gradient Compression (LGC).
Our methods exploit an autoencoder (i.e. trained during the first stages of the
distributed training) to capture the common information that exists in the
gradients of the distributed nodes. We have tested our LGC methods on the image
classification and semantic segmentation tasks using different convolutional
neural networks (ResNet50, ResNet101, PSPNet) and multiple datasets (ImageNet,
Cifar10, CamVid). The ResNet101 model trained for image classification on
Cifar10 achieved an accuracy of 93.57%, which is lower than the baseline
distributed training with uncompressed gradients only by 0.18%.

===========================================================================
Query id: 82
I have a basic understanding of a research direction in Question & Answering (Q&A) systems.
These systems employ an integrated knowledge base, structured similarly to a database, with relational dependencies among various concepts and pre-existing answers.
They interpret user queries and retrieve answers by leveraging the dependencies between concept nodes within the knowledge base.
I am interested in employing a deep neural network to model this type of knowledge base, thereby eliminating the need for hard-coded, rule-based relationships between concepts.
I am curious to see if such a neural network model could overcome the limitations inherent in rule-based knowledge base models.
This is because all relationships and dependencies would have implicit vectorized representations, potentially enhancing computational efficiency.
This approach might also augment the model's expressiveness and capacity to answer more complex questions.
My goal is to demonstrate, using Q&A datasets, that my proposed model outperforms traditional Q&A models in terms of increased accuracy and faster runtime.

Best Candidate ID: 10264, Best Candidate Averaged Relevance Score: 1.154

Best Candidate:
Most existing approaches for Knowledge Base Question Answering (KBQA) focus
on a specific underlying knowledge base either because of inherent assumptions
in the approach, or because evaluating it on a different knowledge base
requires non-trivial changes. However, many popular knowledge bases share
similarities in their underlying schemas that can be leveraged to facilitate
generalization across knowledge bases. To achieve this generalization, we
introduce a KBQA framework based on a 2-stage architecture that explicitly
separates semantic parsing from the knowledge base interaction, facilitating
transfer learning across datasets and knowledge graphs. We show that
pretraining on datasets with a different underlying knowledge base can
nevertheless provide significant performance gains and reduce sample
complexity. Our approach achieves comparable or state-of-the-art performance
for LC-QuAD (DBpedia), WebQSP (Freebase), SimpleQuestions (Wikidata) and MetaQA
(Wikimovies-KG).

===========================================================================
Query id: 90
As a healthcare organization, our goal is to create a system that can effectively extract crucial information from Electronic Health Records (EHRs) to enhance patient care and medical research.
Given the challenge of limited labeled data in EHRs, we intend to utilize transfer learning and cross-lingual methods to train a model for this task.
Consequently, we are interested in identifying tasks that bear the highest similarity to information extraction from EHRs.
Due to privacy concerns, we are unable to test the model on actual EHRs.
Therefore, we require methods to generate datasets that closely resemble our format, using fictitious patient information, to test our model.
Furthermore, since some EHRs may not convert doctors’ notes into text, the model should be capable of interpreting the doctor's handwriting and accurately extracting information from these notes.
We have a preference for pre-trained models that have been benchmarked or models that come with training codes.

Best Candidate ID: 120849, Best Candidate Averaged Relevance Score: 1.154

Best Candidate:
Electronic Health Records (EHRs) are commonly used by the machine learning
community for research on problems specifically related to health care and
medicine. EHRs have the advantages that they can be easily distributed and
contain many features useful for e.g. classification problems. What makes EHR
data sets different from typical machine learning data sets is that they are
often very sparse, due to their high dimensionality, and often contain
heterogeneous (mixed) data types. Furthermore, the data sets deal with
sensitive information, which limits the distribution of any models learned
using them, due to privacy concerns. For these reasons, using EHR data in
practice presents a real challenge. In this work, we explore using Generative
Adversarial Networks to generate synthetic, heterogeneous EHRs with the goal of
using these synthetic records in place of existing data sets for downstream
classification tasks. We will further explore applying differential privacy
(DP) preserving optimization in order to produce DP synthetic EHR data sets,
which provide rigorous privacy guarantees, and are therefore shareable and
usable in the real world. The performance (measured by AUROC, AUPRC and
accuracy) of our model's synthetic, heterogeneous data is very close to the
original data set (within 3 - 5% of the baseline) for the non-DP model when
tested in a binary classification task. Using strong $(1, 10^{-5})$ DP, our
model still produces data useful for machine learning tasks, albeit incurring a
roughly 17% performance penalty in our tested classification task. We
additionally perform a sub-population analysis and find that our model does not
introduce any bias into the synthetic EHR data compared to the baseline in
either male/female populations, or the 0-18, 19-50 and 51+ age groups in terms
of classification performance for either the non-DP or DP variant.

===========================================================================
Query id: 31
I want to develop an AI system that can assist architects in designing buildings.
Therefore, I need a robot agent that can understand the spatial relationships between real-life objects and its current position.
To do that, I think the agent needs to have vision capabilities and a rich set of functional capabilities using state-of-the-art techniques.
Additionally, I hope the model have the ability to build 3D spatial modeling and has a constraint solver.
Also, since the architects are not necessarily experts in computer science, the robot agent needs to have a good user interface.
I hope the agent can provide a vision system for spatial question answering and support speech input/output.

Best Candidate ID: 166971, Best Candidate Averaged Relevance Score: 1.143

Best Candidate:
In this paper we present a system that detects and tracks objects and agents,
computes spatial relations, and communicates those relations to the user using
speech. Our system is able to detect multiple objects and agents at 30 frames
per second using a RGBD camera. It is able to extract the spatial relations in,
on, next to, near, and belongs to, and communicate these relations using
natural language. The notion of belonging is particularly important for
Human-Robot Interaction since it allows the robot ground the language and
reason about the right objects. Although our system is currently static and
targeted to a fixed location in a room, we are planning to port it to a mobile
robot thus allowing it explore the environment and create a spatial knowledge
base.

===========================================================================
Query id: 42
My goal is to build a conversational chatbot that can generate full-length natural language answers to spoken questions.
Because conversations happen in real time, In order to quickly detect the intents in the spoken questions, I do not want to use large scale neural language models.
Instead, I want to utilize heuristics similar to  the word saliency scores which can quickly help the model identify key concepts and intents in a spoken question.
What are some ways to calculate word saliency scores?
After the intents of the spoken question have been established, the model must be able to retrieve facts from a knowledge base, which typically is a graph.
Therefore, the natural language model needs to take a graph as input.

Best Candidate ID: 113972, Best Candidate Averaged Relevance Score: 1.143

Best Candidate:
Pre-trained language models have achieved huge improvement on many NLP tasks.
However, these methods are usually designed for written text, so they do not
consider the properties of spoken language. Therefore, this paper aims at
generalizing the idea of language model pre-training to lattices generated by
recognition systems. We propose a framework that trains neural lattice language
models to provide contextualized representations for spoken language
understanding tasks. The proposed two-stage pre-training approach reduces the
demands of speech data and has better efficiency. Experiments on intent
detection and dialogue act recognition datasets demonstrate that our proposed
method consistently outperforms strong baselines when evaluated on spoken
inputs. The code is available at https://github.com/MiuLab/Lattice-ELMo.

===========================================================================
Query id: 81
I am interested in conducting a study to examine the behavior and learning capabilities of both human and artificial intelligence (AI) agents within a simulated psychology laboratory environment.
This environment must be computerized to allow AI agents to interact within it.
I require a pre-existing platform or a method to construct one that enables the implementation of psychological experiments.
This platform must be compatible with both human subjects and AI agents, necessitating a computerized system with a visual user interface.
The platform should include an Application Programming Interface (API) that allows for the creation of customized tasks and experiments.
Essential built-in functionalities should include but not be limited to object detection, object tracking, and change detection.
Upon acquiring or constructing such a platform, I aim to explore how reinforcement learning (RL) agents perceive and interpret visual information.
I am also interested in investigating how their learning capabilities can be enhanced through the integration of simpler models, such as a basic central vision model.
I hope that this platform and the subsequent research it facilitates can contribute to the advancement of deep RL and strengthen its ties with cognitive science.

Best Candidate ID: 156983, Best Candidate Averaged Relevance Score: 1.143

Best Candidate:
The capability to learn and adapt to changes in the driving environment is
crucial for developing autonomous driving systems that are scalable beyond
geo-fenced operational design domains. Deep Reinforcement Learning (RL)
provides a promising and scalable framework for developing adaptive learning
based solutions. Deep RL methods usually model the problem as a (Partially
Observable) Markov Decision Process in which an agent acts in a stationary
environment to learn an optimal behavior policy. However, driving involves
complex interaction between multiple, intelligent (artificial or human) agents
in a highly non-stationary environment. In this paper, we propose the use of
Partially Observable Markov Games(POSG) for formulating the connected
autonomous driving problems with realistic assumptions. We provide a taxonomy
of multi-agent learning environments based on the nature of tasks, nature of
agents and the nature of the environment to help in categorizing various
autonomous driving problems that can be addressed under the proposed
formulation. As our main contributions, we provide MACAD-Gym, a Multi-Agent
Connected, Autonomous Driving agent learning platform for furthering research
in this direction. Our MACAD-Gym platform provides an extensible set of
Connected Autonomous Driving (CAD) simulation environments that enable the
research and development of Deep RL- based integrated sensing, perception,
planning and control algorithms for CAD systems with unlimited operational
design domain under realistic, multi-agent settings. We also share the
MACAD-Agents that were trained successfully using the MACAD-Gym platform to
learn control policies for multiple vehicle agents in a partially observable,
stop-sign controlled, 3-way urban intersection environment with raw (camera)
sensor observations.

===========================================================================
Query id: 88
As an education researcher, my goal is to create a system that can generate explanations for preschool children as part of their early education.
I require a model that can provide real-time explanations for the problems inputted.
Given that the target users are preschool children, the model needs to be capable of accepting verbal inputs, integrating phrases into natural language, and predicting the user's question.
One challenge I face is that adults rarely discuss common sense and trivial questions, so I require an additional dataset to address these types of questions.
It is crucial that the content in the dataset is thoroughly vetted by authorities, as the model will be used by children.
The model also needs the ability to discard any inappropriate knowledge it may have previously learned.
If possible, I would also like guidance on how to strike a balance between the response time of the model and the size of the dataset.

Best Candidate ID: 40109, Best Candidate Averaged Relevance Score: 1.133

Best Candidate:
There is a growing interest in designing autonomous agents that can work
alongside humans. Such agents will undoubtedly be expected to explain their
behavior and decisions. While generating explanations is an actively researched
topic, most works tend to focus on methods that generate explanations that are
one size fits all. As in the specifics of the user-model are completely
ignored. The handful of works that look at tailoring their explanation to the
user's background rely on having specific models of the users (either analytic
models or learned labeling models). The goal of this work is thus to propose an
end-to-end adaptive explanation generation system that begins by learning the
different types of users that the agent could interact with. Then during the
interaction with the target user, it is tasked with identifying the type on the
fly and adjust its explanations accordingly. The former is achieved by a
data-driven clustering approach while for the latter, we compile our
explanation generation problem into a POMDP. We demonstrate the usefulness of
our system on two domains using state-of-the-art POMDP solvers. We also report
the results of a user study that investigates the benefits of providing
personalized explanations in a human-robot interaction setting.

===========================================================================
Query id: 44
I want to build a healthcare decision support system that will recommend treatment plans for patients.
In order to make the prediction process transparent and interpretable, I believe I could use reinforcement learning.
Given the patient’s past medical history, I will spawn many processes, in each process an artificial or digital twin for the patient is spawned to mimic the patient’s health trajectories.
In each process, the patient will receive many treatments, prescriptions, surgeries, and will experience a rest and recovery period, and eventually some form of complications or disease will terminate the patient, and thus terminate the process as well.
I want to model this entire environment dynamic using reinforcement learning, conditioned on the patient's past medical record.
I will model each dynamic in the environment as some form of numeric optimization problems where the human body strives to optimize its ability to recover and detect possible illness.
I need the entire framework to be effective in casting optimization problems, and efficiently solve numeric planning problems involving obstacles.
I will use this modeling approach to determine the best course of treatment for the patient to maximize the patient’s life expectancy.

Best Candidate ID: 12973, Best Candidate Averaged Relevance Score: 1.118

Best Candidate:
Although reinforcement learning (RL) has tremendous success in many fields,
applying RL to real-world settings such as healthcare is challenging when the
reward is hard to specify and no exploration is allowed. In this work, we focus
on recovering clinicians' rewards in treating patients. We incorporate the
what-if reasoning to explain the clinician's treatments based on their
potential future outcomes. We use generalized additive models (GAMs) - a class
of accurate, interpretable models - to recover the reward. In both simulation
and a real-world hospital dataset, we show our model outperforms baselines.
Finally, our model's explanations match several clinical guidelines when
treating patients while we found the commonly-used linear model often
contradicts them.

===========================================================================
Query id: 60
I aim to develop a neural machine translation (NMT) model capable of effectively translating between multiple languages, each with unique morpheme usage.
I wish to address several limitations of traditional NMT models.
Firstly, my model should not rely on a fixed vocabulary but rather be capable of processing, embedding, and understanding new, unseen words.
To achieve this, I plan to implement an intelligent subword tokenization scheme, which should enable my model to surpass the performance of basic character-level NMT.
Moreover, the model needs to be parameter-efficient to facilitate deployment on compact wearable devices.
It should also be capable of managing long-distance context and grammar dependencies within paragraphs.
My current approach involves using a form of hierarchical decoding.
This method initially translates larger and longer word or phrase-level information swiftly, then fine-tunes subword or even character-level information with greater precision.
My model needs to reach state-of-the-art performance on standard machine translation datasets.

Best Candidate ID: 233263, Best Candidate Averaged Relevance Score: 1.118

Best Candidate:
In neural machine translation (NMT), researchers face the challenge of
un-seen (or out-of-vocabulary OOV) words translation. To solve this, some
researchers propose the splitting of western languages such as English and
German into sub-words or compounds. In this paper, we try to address this OOV
issue and improve the NMT adequacy with a harder language Chinese whose
characters are even more sophisticated in composition. We integrate the Chinese
radicals into the NMT model with different settings to address the unseen words
challenge in Chinese to English translation. On the other hand, this also can
be considered as semantic part of the MT system since the Chinese radicals
usually carry the essential meaning of the words they are constructed in.
Meaningful radicals and new characters can be integrated into the NMT systems
with our models. We use an attention-based NMT system as a strong baseline
system. The experiments on standard Chinese-to-English NIST translation shared
task data 2006 and 2008 show that our designed models outperform the baseline
model in a wide range of state-of-the-art evaluation metrics including LEPOR,
BEER, and CharacTER, in addition to BLEU and NIST scores, especially on the
adequacy-level translation. We also have some interesting findings from the
results of our various experiment settings about the performance of words and
characters in Chinese NMT, which is different with other languages. For
instance, the fully character level NMT may perform well or the state of the
art in some other languages as researchers demonstrated recently, however, in
the Chinese NMT model, word boundary knowledge is important for the model
learning.

===========================================================================
Query id: 58
As a data scientist, I am currently developing an early detection system designed to identify the onset of progressive neurodegenerative diseases through the analysis of brain scans.
My current dataset consists of a large volume of unlabeled data, supplemented by a limited number of labeled positive examples.
My goal is to create a model that can effectively learn from this data to ensure accurate detection.
Rather than relying solely on importance reweighting during the model training phase, I am aiming to create a self-learning model.
This would involve the model setting its own learning pace, determining its own loss, and gaining insights from its own performance during the training process.
As such, the incorporation of some form of reinforcement learning might be necessary.
If I go down the route of using RL, given the nature of my dataset, I am particularly interested in reinforcement learning algorithms for hyperparameter selection that can learn from sparse reward signals.
Ultimately, my goal is for my trained model to outperform other methods when applied to specific datasets related to progressive neurodegenerative diseases, particularly those containing brain images.

Best Candidate ID: 162384, Best Candidate Averaged Relevance Score: 1.083

Best Candidate:
Deep learning is attracting significant interest in the neuroimaging
community as a means to diagnose psychiatric and neurological disorders from
structural magnetic resonance images. However, there is a tendency amongst
researchers to adopt architectures optimized for traditional computer vision
tasks, rather than design networks customized for neuroimaging data. We address
this by introducing NEURO-DRAM, a 3D recurrent visual attention model tailored
for neuroimaging classification. The model comprises an agent which, trained by
reinforcement learning, learns to navigate through volumetric images,
selectively attending to the most informative regions for a given task. When
applied to Alzheimer's disease prediction, NEURODRAM achieves state-of-the-art
classification accuracy on an out-of-sample dataset, significantly
outperforming a baseline convolutional neural network. When further applied to
the task of predicting which patients with mild cognitive impairment will be
diagnosed with Alzheimer's disease within two years, the model achieves
state-of-the-art accuracy with no additional training. Encouragingly, the agent
learns, without explicit instruction, a search policy in agreement with
standardized radiological hallmarks of Alzheimer's disease, suggesting a route
to automated biomarker discovery for more poorly understood disorders.

===========================================================================
Query id: 84
I find that the primary constraint in modern artificial intelligence applications is the computational resources expended on multilayer perceptrons (MLP) within neural networks.
To enhance the efficiency of MLP, I am seeking a method to distill MLP by eliminating non-essential weights.
To accomplish this, I require a theoretical understanding of the weights in shallow MLPs and an empirical application of these theories to deeper neural networks.
Furthermore, I need a metric that takes into account both the computational resources saved and the performance loss resulting from the distillation.
I understand that the concept of sparsity could be beneficial in representing the number of edges in a graph, but I need guidance on how to apply this concept to MLPs, which can be viewed as a specific type of graph.

Best Candidate ID: 74027, Best Candidate Averaged Relevance Score: 1.083

Best Candidate:
In this work, we firstly apply the Train-Tensor (TT) networks to construct a
compact representation of the classical Multilayer Perceptron, representing a
reduction of up to 95% of the coefficients. A comparative analysis between
tensor model and standard multilayer neural networks is also carried out in the
context of prediction of the Mackey-Glass noisy chaotic time series and NASDAQ
index. We show that the weights of a multidimensional regression model can be
learned by means of TT network and the optimization of TT weights is a more
robust to the impact of coefficient initialization and hyper-parameter setting.
Furthermore, an efficient algorithm based on alternating least squares has been
proposed for approximating the weights in TT-format with a reduction of
computational calculus, providing a much faster convergence than the well-known
adaptive learning-method algorithms, widely applied for optimizing neural
networks.

===========================================================================
Query id: 12
In machine translation tasks, when the source and target languages contain sentences of vastly different levels of complexity, many neural machine translation models do not perform as well.
For example, if I want to translate very technical medical documents in English to more colloquial and simpler explanations in Swahili, current translation models do not work well.
To address this issue, I am looking to develop tools that can simplify complex sentences in a parallel fashion.
I also want to develop a neural machine translation tool that considers the difference in text complexity levels between the source and target language of a translation corpus.
Of course, since we cannot humanly annotate the level of difficulty for each sentence, I only want to look at unsupervised approaches.
I am also interested in creating a large-scale pseudo dataset using an unsupervised approach, where I make sure the aligned sentences preserve the same meanings and sentence pairs with a higher complexity difference are kept.
I hope this pseudo data could improve the performance of my models.
What would be a good way to model sentence complexity in an unsupervised setting and what would be a good way to create such a pseudo dataset?

Best Candidate ID: 55293, Best Candidate Averaged Relevance Score: 1.071

Best Candidate:
Objective: Today's neural machine translation (NMT) can achieve near
human-level translation quality and greatly facilitates international
communications, but the lack of parallel corpora poses a key problem to the
development of translation systems for highly specialized domains, such as
biomedicine. This work presents an unsupervised algorithm for deriving parallel
corpora from document-level translations by using sentence alignment and
explores how training materials affect the performance of biomedical NMT
systems. Materials and Methods: Document-level translations are mixed to train
bilingual word embeddings (BWEs) for the evaluation of cross-lingual word
similarity, and sentence distance is defined by combining semantic and
positional similarities of the sentences. The alignment of sentences is
formulated as an extended earth mover's distance problem. A Chinese-English
biomedical parallel corpus is derived with the proposed algorithm using
bilingual articles from UpToDate and translations of PubMed abstracts, which is
then used for the training and evaluation of NMT. Results: On two manually
aligned translation datasets, the proposed algorithm achieved accurate sentence
alignment in the 1-to-1 cases and outperformed competing algorithms in the
many-to-many cases. The NMT model fine-tuned on biomedical data significantly
improved the in-domain translation quality (zh-en: +17.72 BLEU; en-zh: +17.02
BLEU). Both the size of the training data and the combination of different
corpora can significantly affect the model's performance. Conclusion: The
proposed algorithm relaxes the assumption for sentence alignment and
effectively generates accurate translation pairs that facilitate training high
quality biomedical NMT models.

===========================================================================
Query id: 40
I am planning to develop a chit-chat conversation system that can be used in customer service chatbots to provide personalized and accurate responses to customer inquiries.
In order for the chatbot to be empathetic to individual customer’s needs, I am looking for models that incorporate persona perception and mutual persona perception, to have a comprehensive understanding of word meaning and can identify lexical ambiguity.
I think psycholinguistic theories of the mental lexicon can help in developing such models, so I also need some application examples of such theories in the NLP field.
To test my future model, I also need datasets of various conversations with human annotation so that I can test the model under different cases.
Since the expected number of customers that need the service at the same time may be large, I also need the system to perform well under multithreading with limited computation resources.

Best Candidate ID: 170492, Best Candidate Averaged Relevance Score: 1.053

Best Candidate:
A conversational agent (chatbot) is a piece of software that is able to
communicate with humans using natural language. Modeling conversation is an
important task in natural language processing and artificial intelligence.
While chatbots can be used for various tasks, in general they have to
understand users' utterances and provide responses that are relevant to the
problem at hand.
  In my work, I conduct an in-depth survey of recent literature, examining over
70 publications related to chatbots published in the last 3 years. Then, I
proceed to make the argument that the very nature of the general conversation
domain demands approaches that are different from current state-of-of-the-art
architectures. Based on several examples from the literature I show why current
chatbot models fail to take into account enough priors when generating
responses and how this affects the quality of the conversation. In the case of
chatbots, these priors can be outside sources of information that the
conversation is conditioned on like the persona or mood of the conversers. In
addition to presenting the reasons behind this problem, I propose several ideas
on how it could be remedied.
  The next section focuses on adapting the very recent Transformer model to the
chatbot domain, which is currently state-of-the-art in neural machine
translation. I first present experiments with the vanilla model, using
conversations extracted from the Cornell Movie-Dialog Corpus. Secondly, I
augment the model with some of my ideas regarding the issues of encoder-decoder
architectures. More specifically, I feed additional features into the model
like mood or persona together with the raw conversation data. Finally, I
conduct a detailed analysis of how the vanilla model performs on conversational
data by comparing it to previous chatbot models and how the additional features
affect the quality of the generated responses.

===========================================================================
Query id: 32
I want to make an analysis about the users’ behavior in a social media platform.
With such analysis, I can find more effective marketing strategies.
Since the platform allows the users to use informal and abbreviated language like cuz or LOL, I need a model that can identify informal language, slang and abbreviated words and match them with their formal explanation, so that I can make a further analysis based on the pre-processed data.
I am looking for text mining techniques that can be used in the model.
Also, the accuracy of the identification and interpretation of informal languages is crucial, and it would be best if the model can extract users’ intention from the text.
I am looking for multiple algorithms that can be used in such models so that I can compare the effectiveness and evaluate their accuracy in my dataset.

Best Candidate ID: 40001, Best Candidate Averaged Relevance Score: 1.0

Best Candidate:
The interest in demographic information retrieval based on text data has
increased in the research community because applications have shown success in
different sectors such as security, marketing, heath-care, and others.
Recognition and identification of demographic traits such as gender, age,
location, or personality based on text data can help to improve different
marketing strategies. For instance it makes it possible to segment and to
personalize offers, thus products and services are exposed to the group of
greatest interest. This type of technology has been discussed widely in
documents from social media. However, the methods have been poorly studied in
data with a more formal structure, where there is no access to emoticons,
mentions, and other linguistic phenomena that are only present in social media.
This paper proposes the use of recurrent and convolutional neural networks, and
a transfer learning strategy for gender recognition in documents that are
written in informal and formal languages. Models are tested in two different
databases consisting of Tweets and call-center conversations. Accuracies of up
to 75\% are achieved for both databases. The results also indicate that it is
possible to transfer the knowledge from a system trained on a specific type of
expressions or idioms such as those typically used in social media into a more
formal type of text data, where the amount of data is more scarce and its
structure is completely different.

===========================================================================
Query id: 39
As a professional game player, I am interested in reinforcement learning algorithms.
However, some of the algorithms simply start training without a good initialization, and I am looking for some different algorithms.
I heard there is a technique that is called behavioral cloning, which allows the data to learn from good players initially.
The model should be able to identify and record data from humans accurately and minimize the impact of human reflexes.
I have tons of videos of myself playing dozens of games that I am quite familiar with and good at.
These videos may work as good initialization.
Then, I hope the model can use traditional reinforcement learning strategies to send keystrokes to a game, analyze game mechanics, and make better decisions.
Since I can provide a good initialization, I hope the model does not need to modify the game and can achieve better performance at the same time when compared to other models that are initialized randomly.
I hope such models can provide inspiration to me to play the game better in the future.

Best Candidate ID: 209926, Best Candidate Averaged Relevance Score: 1.0

Best Candidate:
To solve complex real-world problems with reinforcement learning, we cannot
rely on manually specified reward functions. Instead, we can have humans
communicate an objective to the agent directly. In this work, we combine two
approaches to learning from human feedback: expert demonstrations and
trajectory preferences. We train a deep neural network to model the reward
function and use its predicted reward to train an DQN-based deep reinforcement
learning agent on 9 Atari games. Our approach beats the imitation learning
baseline in 7 games and achieves strictly superhuman performance on 2 games
without using game rewards. Additionally, we investigate the goodness of fit of
the reward model, present some reward hacking problems, and study the effects
of noise in the human labels.

===========================================================================
Query id: 91
I am currently developing a transparent and reliable AI chatbot.
I am in search of AI models that can complete incomplete sentences or answer questions based on the next token prediction.
For transparency, I need models that can explicitly compute the probability of the next token and update it in real-time based on user input.
I also aim to provide a visualization that displays the perceptrons and weights utilized during each inference to the user.
To ensure trustworthiness, I require techniques that can perturb the training set to increase the model's resilience against adversarial attacks.
The model should also possess the ability to forget or replace outdated knowledge, as some data from the training set may become incorrect over time.
Considering that a significant portion of potential training data from the internet includes sensitive personal information, I also require an efficient filter to automatically mask such information prior to its use in training.
Ultimately, my objective is to develop an AI chatbot that is both transparent and trustworthy.

Best Candidate ID: 27784, Best Candidate Averaged Relevance Score: 0.933

Best Candidate:
When recurrent neural network transducers (RNNTs) are trained using the
typical maximum likelihood criterion, the prediction network is trained only on
ground truth label sequences. This leads to a mismatch during inference, known
as exposure bias, when the model must deal with label sequences containing
errors. In this paper we investigate approaches to reducing exposure bias in
training to improve the generalization of RNNT models for automatic speech
recognition (ASR). A label-preserving input perturbation to the prediction
network is introduced. The input token sequences are perturbed using SwitchOut
and scheduled sampling based on an additional token language model. Experiments
conducted on the 300-hour Switchboard dataset demonstrate their effectiveness.
By reducing the exposure bias, we show that we can further improve the accuracy
of a high-performance RNNT ASR model and obtain state-of-the-art results on the
300-hour Switchboard dataset.

===========================================================================
Query id: 74
I am currently engaged in a project that involves identifying and isolating concealed objects in videos, with a particular emphasis on tracking their movements.
I am encountering difficulties in consistently outlining the edges of camouflaged objects, especially when certain parts of the video exhibit no movement.
I require a system capable of aligning video frames based on the background while simultaneously tracking target objects and ensuring consistent tracking.
The intended application of this model is in nature reserves, where it will be used to monitor the living conditions of endangered species.
Therefore, I am seeking a model that can effectively handle various animal types and accurately label them during video storage.
Ideally, the model should be trained on a comprehensive dataset encompassing a broad spectrum of animal species.
Furthermore, I am interested in understanding how to apply supervised training models to unsupervised tasks, given that some of the animals may be too rare to be included in the training dataset.

Best Candidate ID: 136248, Best Candidate Averaged Relevance Score: 0.923

Best Candidate:
Object tracking is one of the most important problems in computer vision. The
aim of video tracking is to extract the trajectories of a target or object of
interest, i.e. accurately locate a moving target in a video sequence and
discriminate target from non-targets in the feature space of the sequence. So,
feature descriptors can have significant effects on such discrimination. In
this paper, we use the basic idea of many trackers which consists of three main
components of the reference model, i.e., object modeling, object detection and
localization, and model updating. However, there are major improvements in our
system. Our forth component, occlusion handling, utilizes the r-spatiogram to
detect the best target candidate. While spatiogram contains some moments upon
the coordinates of the pixels, r-spatiogram computes region-based compactness
on the distribution of the given feature in the image that captures richer
features to represent the objects. The proposed research develops an efficient
and robust way to keep tracking the object throughout video sequences in the
presence of significant appearance variations and severe occlusions. The
proposed method is evaluated on the Princeton RGBD tracking dataset considering
sequences with different challenges and the obtained results demonstrate the
effectiveness of the proposed method.

===========================================================================
Query id: 18
I am developing an online grading system for undergraduate biology classrooms to make the biology instructor’s work easier.
The system will provide a web portal that allows me to upload a response file to different assessing reasoning.
I am looking for some model that can analyze student responses automatically by identifying naive concepts within each sentence of the response and then provide detailed information about the student’s ideas within each response.
Based on the concepts from the students’ responses, I will then try to find some technique to allow  my system to score the students’ assignments automatically.
Also, I need another model that can provide detailed analysis of the students’ overall reasoning types so that I can understand the student’s confusion about the assignment and improve my instruction quality.

Best Candidate ID: 223148, Best Candidate Averaged Relevance Score: 0.909

Best Candidate:
Automatic grading is not a new approach but the need to adapt the latest
technology to automatic grading has become very important. As the technology
has rapidly became more powerful on scoring exams and essays, especially from
the 1990s onwards, partially or wholly automated grading systems using
computational methods have evolved and have become a major area of research. In
particular, the demand of scoring of natural language responses has created a
need for tools that can be applied to automatically grade these responses. In
this paper, we focus on the concept of automatic grading of short answer
questions such as are typical in the UK GCSE system, and providing useful
feedback on their answers to students. We present experimental results on a
dataset provided from the introductory computer science class in the University
of North Texas. We first apply standard data mining techniques to the corpus of
student answers for the purpose of measuring similarity between the student
answers and the model answer. This is based on the number of common words. We
then evaluate the relation between these similarities and marks awarded by
scorers. We then consider an approach that groups student answers into
clusters. Each cluster would be awarded the same mark, and the same feedback
given to each answer in a cluster. In this manner, we demonstrate that clusters
indicate the groups of students who are awarded the same or the similar scores.
Words in each cluster are compared to show that clusters are constructed based
on how many and which words of the model answer have been used. The main
novelty in this paper is that we design a model to predict marks based on the
similarities between the student answers and the model answer.

===========================================================================
Query id: 93
I aim to create a chatbot capable of interpreting images as input and analyzing them for subsequent tasks.
The model should deliver responses efficiently without compromising the quality of the answers.
To achieve this, I require an image analysis model that can swiftly perform image classification by ignoring unnecessary local detailed features.
Additionally, the model should be capable of executing pixel-level localization and analysis as required.
To enhance efficiency, I plan to incorporate reinforcement learning for user prediction.
I envision the chatbot predicting the user's potential next questions, allowing the image analysis model to analyze relevant images while awaiting user input.
The reward component of the framework should be directly linked to the accuracy of the prediction and the time saved.
Furthermore, I want the method to remember the most frequently asked questions related to popular images, enabling it to provide immediate answers to these queries.

Best Candidate ID: 95096, Best Candidate Averaged Relevance Score: 0.9

Best Candidate:
The accuracy of deep convolutional neural networks (CNNs) generally improves
when fueled with high resolution images. However, this often comes at a high
computational cost and high memory footprint. Inspired by the fact that not all
regions in an image are task-relevant, we propose a novel framework that
performs efficient image classification by processing a sequence of relatively
small inputs, which are strategically selected from the original image with
reinforcement learning. Such a dynamic decision process naturally facilitates
adaptive inference at test time, i.e., it can be terminated once the model is
sufficiently confident about its prediction and thus avoids further redundant
computation. Notably, our framework is general and flexible as it is compatible
with most of the state-of-the-art light-weighted CNNs (such as MobileNets,
EfficientNets and RegNets), which can be conveniently deployed as the backbone
feature extractor. Experiments on ImageNet show that our method consistently
improves the computational efficiency of a wide variety of deep models. For
example, it further reduces the average latency of the highly efficient
MobileNet-V3 on an iPhone XS Max by 20% without sacrificing accuracy. Code and
pre-trained models are available at
https://github.com/blackfeather-wang/GFNet-Pytorch.

===========================================================================
Query id: 21
I am a mineral processing engineer and I am working on a system that could improve the efficiency of mineral production.
To do so, I need to control the amount of iron ore, fed into the production pipeline as raw materials, so that the final mineral product has high quality.
Therefore, I need to be able to gauge and measure the characteristics of the iron ore pellets in real time.
I am thinking about using two neural network models.
One model will detect and determine the features of the iron ore pellets based on 3D images, so it will be a computer vision model.
The second model will use these estimated features to predict the best load amount of iron ore feeding into the pipeline.
What model should I use?
However, I don’t have labels for these iron ore pellets, so I am exploring the unsupervised or weakly supervised settings.
In the end, the system  must perform real-time feed load estimation and can handle large-scale iron ore processing.

Best Candidate ID: 20823, Best Candidate Averaged Relevance Score: 0.889

Best Candidate:
Consistency in product quality is of critical importance in manufacturing.
However, achieving a target product quality typically involves balancing a
large number of manufacturing attributes. Existing manufacturing practices for
dealing with such complexity are driven largely based on human knowledge and
experience. The prevalence of manual intervention makes it difficult to perfect
manufacturing practices, underscoring the need for a data-driven solution. In
this paper, we present an Industrial Internet of Things (IIoT) machine model
which enables effective monitoring and control of plant machinery so as to
achieve consistency in product quality. We present algorithms that can provide
product quality prediction during production, and provide recommendations for
machine control. Subsequently, we perform an experimental evaluation of the
proposed solution using real data captured from a food processing plant. We
show that the proposed algorithms can be used to predict product quality with a
high degree of accuracy, thereby enabling effective production monitoring and
control.

===========================================================================
Query id: 43
As an Airbus consultant, my goal is to improve and enhance the user experience in a large commercial plane.
I need a model that can not only complete traditional tasks like identity validation and security check but also help to understand people’s emotions and make them feel better while using the transportation system.
I need a model that can collect and analyze positioning and ticket validation data.
The model, I hope, should also be able to identify people’s emotional state through facial recognition and gesture tracking during the same process.
To make people feel better, I need the model to pass the emotion analysis to the musical interface and generate appropriate sound in real-time, adapting to the user’s emotional state and manipulating musical elements accordingly.
I hope the model can also help to point out the reasons people’s moods change, and we can thus use these data to improve our service during flight.

Best Candidate ID: 38864, Best Candidate Averaged Relevance Score: 0.8

Best Candidate:
People have the ability to make sensible assumptions about other people's
emotional states by being sympathetic, and because of our common sense of
knowledge and the ability to think visually. Over the years, much research has
been done on providing machines with the ability to detect human emotions and
to develop automated emotional intelligence systems. The computer's ability to
detect human emotions is gaining popularity in creating sensitive systems such
as learning environments, health care systems and real-world. Improving
people's health has been the subject of much research. This paper describes the
formation as conceptual evidence of emotional acquisition and control in
intelligent health settings. The authors of this paper aim for an
unconventional approach with a friendly look to get emotional scenarios from
the system to establish a functional, non-intrusive and emotionally-sensitive
environment where users can do their normal activities naturally and see the
program only when pleasant mood activating services are received. The
context-sensitive system interacts with users to detect and differentiate
emotions through facial expressions or speech recognition, to make music
recommendations and mood color treatments with the services installed on their
IoT devices.

===========================================================================
Query id: 45
We find that current facial recognition models have problems when recognizing faces with stripe patterns, such as face camouflage or due to noises in camera devices.
Thus, we need a model that can take human faces with stripe patterns as input, accurately remove the stripe pattern and output the recovered human face.
The model should consider the mathematical relationship between the embeddings of different stripe patterns.
It is essential for the system to maintain image quality after the pattern removal.
I hope the system can take additional images of the same person to improve the quality of the recovered face after removing the stripe pattern.
After removing the stripe pattern, I hope the output image can be directly passed into a facial recognition model and make the whole pipeline end-to-end.

Best Candidate ID: 244684, Best Candidate Averaged Relevance Score: 0.769

Best Candidate:
Motion blur, out of focus, insufficient spatial resolution, lossy compression
and many other factors can all cause an image to have poor quality. However,
image quality is a largely ignored issue in traditional pattern recognition
literature. In this paper, we use face detection and recognition as case
studies to show that image quality is an essential factor which will affect the
performances of traditional algorithms. We demonstrated that it is not the
image quality itself that is the most important, but rather the quality of the
images in the training set should have similar quality as those in the testing
set. To handle real-world application scenarios where images with different
kinds and severities of degradation can be presented to the system, we have
developed a quality classified image analysis framework to deal with images of
mixed qualities adaptively. We use deep neural networks first to classify
images based on their quality classes and then design a separate face detector
and recognizer for images in each quality class. We will present experimental
results to show that our quality classified framework can accurately classify
images based on the type and severity of image degradations and can
significantly boost the performances of state-of-the-art face detector and
recognizer in dealing with image datasets containing mixed quality images.

===========================================================================
Query id: 41
Working for the law enforcement agency, I plan on building a model that can track social media’s chatter for potential illegal activities involving the selling, acquisition, manufacturing of chemical products, as well as identifying the dissemination of dangerous chemistry knowledge that could be used for illegal activities.
Similar to the idea of building a toxic language classifier to identify cyber bullying, I want to build a classification model that can detect potential illegal activities involving chemistry, possibly by building a synthetic dataset for data augmentation purposes.
To build such a synthetic dataset accurately, I need a large knowledge base for chemistry, so I plan on creating a model to effectively identify trends in chemical literature, extract detailed knowledge about chemical reactions.
The same model could also be used to analyze complex names mentioned in social media chatter, and interpret graphic representations of entities in criminal enterprises, such as buyers, sellers, distributors, enforcers.
Since this system will be used for law enforcement, so it needs to be transparent, therefore, all the classifications and decisions it makes need to contain causal explanations.

Best Candidate ID: 127378, Best Candidate Averaged Relevance Score: 0.739

Best Candidate:
The goal of this paper is to summarize methodologies used in extracting
entities and topics from a database of criminal records and from a database of
newspapers. Statistical models had successfully been used in studying the
topics of roughly 300,000 New York Times articles. In addition, these models
had also been used to successfully analyze entities related to people,
organizations, and places (D Newman, 2006). Additionally, analytical
approaches, especially in hotspot mapping, were used in some researches with an
aim to predict crime locations and circumstances in the future, and those
approaches had been tested quite successfully (S Chainey, 2008). Based on the
two above notions, this research was performed with the intention to apply data
science techniques in analyzing a big amount of data, selecting valuable
intelligence, clustering violations depending on their types of crime, and
creating a crime graph that changes through time. In this research, the task
was to download criminal datasets from Kaggle and a collection of news articles
from Kaggle and EAGER project databases, and then to merge these datasets into
one general dataset. The most important goal of this project was performing
statistical and natural language processing methods to extract entities and
topics as well as to group similar data points into correct clusters, in order
to understand public data about U.S related crimes better.

===========================================================================
Query id: 98
As an environmental researcher, I aim to create a mobile application that uses generative adversarial networks (GANs) to produce personalized, vivid images illustrating the potential impacts of climate change on specific locations.
My plan is to train the GAN on street-view images of horses before and after extreme weather events.
However, since such a dataset does not exist, I require a model capable of learning the 3D structure of an object from images taken from various angles and subsequently rendering an image of the structure from a specified direction.
Furthermore, I intend for the application to incorporate climate models to evaluate the probability and type of long-term climate-related events.
As I am developing a mobile application, I also need techniques to reduce the latency between the mobile device and the backend server, as well as strategies to manage multiple simultaneous calls to the backend server.
Lastly, I am interested in video compression algorithms that can enhance the upload speed and alleviate the storage pressure on the server.

Best Candidate ID: 163998, Best Candidate Averaged Relevance Score: 0.667

Best Candidate:
In this paper we present, to the best of our knowledge, the first method to
learn a generative model of 3D shapes from natural images in a fully
unsupervised way. For example, we do not use any ground truth 3D or 2D
annotations, stereo video, and ego-motion during the training. Our approach
follows the general strategy of Generative Adversarial Networks, where an image
generator network learns to create image samples that are realistic enough to
fool a discriminator network into believing that they are natural images. In
contrast, in our approach the image generation is split into 2 stages. In the
first stage a generator network outputs 3D objects. In the second, a
differentiable renderer produces an image of the 3D objects from random
viewpoints. The key observation is that a realistic 3D object should yield a
realistic rendering from any plausible viewpoint. Thus, by randomizing the
choice of the viewpoint our proposed training forces the generator network to
learn an interpretable 3D representation disentangled from the viewpoint. In
this work, a 3D representation consists of a triangle mesh and a texture map
that is used to color the triangle surface by using the UV-mapping technique.
We provide analysis of our learning approach, expose its ambiguities and show
how to overcome them. Experimentally, we demonstrate that our method can learn
realistic 3D shapes of faces by using only the natural images of the FFHQ
dataset.

===========================================================================
